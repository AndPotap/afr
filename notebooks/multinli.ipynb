{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env OMP_NUM_THREADS=4\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Modify to your own data dir ###\n",
    "mnli_data_dir = '/data/users/pavel_i/datasets/multinli'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../wilds_exps_utils/\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import pickle\n",
    "import copy\n",
    "from types import SimpleNamespace\n",
    "from wilds import get_dataset\n",
    "from wilds.common.data_loaders import get_eval_loader\n",
    "from wilds_configs import datasets as dataset_configs\n",
    "from wilds.datasets.wilds_dataset import WILDSSubset\n",
    "from wilds_models.initializer import initialize_model\n",
    "import wilds_transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from types import SimpleNamespace\n",
    "import tqdm\n",
    "import torch\n",
    "\n",
    "from transformers import BertConfig, BertForSequenceClassification\n",
    "\n",
    "import sys\n",
    "gdro_dir = '../gdro_fork'\n",
    "sys.path.append(gdro_dir)\n",
    "from gdro_fork.data.data import prepare_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GROUPS = 6\n",
    "\n",
    "gdro_config = SimpleNamespace(\n",
    "    dataset='MultiNLI',\n",
    "    shift_type='confounder',\n",
    "    root_dir=mnli_data_dir,\n",
    "    augment_data=False,\n",
    "    gamma=0.1,\n",
    "    batch_size=128,\n",
    "    target_name='gold_label_random',\n",
    "    confounder_names=['sentence2_has_negation',],\n",
    "    model='bert',\n",
    "    fraction=1.,\n",
    ")\n",
    "reweighting_data, val_data, test_data = prepare_data(gdro_config, train=True)\n",
    "loader_kwargs = {'num_workers':4, 'pin_memory':True}\n",
    "\n",
    "# reweighting_data = val_data\n",
    "val_loader = val_data.get_loader(\n",
    "        train=False, reweight_groups=None, **loader_kwargs)\n",
    "test_loader = test_data.get_loader(\n",
    "        train=False, reweight_groups=None, **loader_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_predictions(feature_extractor, classifier, loader):\n",
    "    all_embeddings, all_predictions, all_y_true, all_metadata = [], [], [], []\n",
    "#     i = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y_true, metadata in tqdm.tqdm(loader):\n",
    "            input_ids = x[:, :, 0].cuda()\n",
    "            input_masks = x[:, :, 1].cuda()\n",
    "            segment_ids = x[:, :, 2].cuda()\n",
    "            embeddings = feature_extractor(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=input_masks,\n",
    "                    token_type_ids=segment_ids).logits\n",
    "            predictions = torch.argmax(classifier(embeddings), axis=1)\n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "            all_predictions.append(predictions.cpu())\n",
    "            all_y_true.append(y_true.cpu())\n",
    "            all_metadata.append(metadata)\n",
    "    all_embeddings = torch.cat(all_embeddings, axis=0)\n",
    "    all_predictions = torch.cat(all_predictions, axis=0)\n",
    "    all_y_true = torch.cat(all_y_true, axis=0)\n",
    "    all_metadata = torch.cat(all_metadata, axis=0)\n",
    "    return all_embeddings, all_predictions, all_y_true, all_metadata\n",
    "\n",
    "def save_emb(ckpt_path, seed, save_path):\n",
    "    reweighting_data, val_data, test_data = prepare_data(gdro_config, train=True)\n",
    "    loader_kwargs = {'num_workers':4, 'pin_memory':True}\n",
    "\n",
    "    # reweighting_data = val_data\n",
    "    val_loader = val_data.get_loader(\n",
    "            train=False, reweight_groups=None, **loader_kwargs)\n",
    "    test_loader = test_data.get_loader(\n",
    "            train=False, reweight_groups=None, **loader_kwargs)\n",
    "    reweighting_seed = seed\n",
    "    reweighting_frac = 0.2\n",
    "\n",
    "    print(f'Dropping reweighting data, seed {reweighting_seed}')\n",
    "\n",
    "    idx = reweighting_data.dataset.indices.copy()\n",
    "    rng = np.random.default_rng(reweighting_seed)\n",
    "    rng.shuffle(idx)\n",
    "    n_train = int((1 - reweighting_frac) * len(idx))\n",
    "    reweighting_idx = idx[n_train:]\n",
    "\n",
    "    print(f'Original dataset size: {len(reweighting_data.dataset.indices)}')\n",
    "    reweighting_data.dataset = torch.utils.data.dataset.Subset(\n",
    "        reweighting_data.dataset.dataset,\n",
    "        indices=reweighting_idx)\n",
    "    print(f'New dataset size: {len(reweighting_data.dataset.indices)}')\n",
    "\n",
    "    reweighting_loader = reweighting_data.get_loader(\n",
    "            train=False, reweight_groups=None, **loader_kwargs)\n",
    "    model = torch.load(ckpt_path)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    classifier = model.classifier\n",
    "    model.classifier = torch.nn.Identity(classifier.in_features)\n",
    "\n",
    "    feature_extractor, classifier = model, classifier\n",
    "    reweighting_embeddings, reweighting_predictions, reweighting_y, reweighting_metadata = get_embeddings_predictions(\n",
    "            feature_extractor, classifier, reweighting_loader)\n",
    "    val_embeddings, val_predictions, val_y, val_metadata = get_embeddings_predictions(\n",
    "            feature_extractor, classifier, val_loader)\n",
    "    test_embeddings, test_predictions, test_y, test_metadata = get_embeddings_predictions(\n",
    "            feature_extractor, classifier, test_loader)\n",
    "    torch.save(\n",
    "        dict(\n",
    "            e=reweighting_embeddings, y=reweighting_y, pred=reweighting_predictions, m=reweighting_metadata,\n",
    "            test_e=test_embeddings, test_pred=test_predictions, test_y=test_y, test_m=test_metadata,\n",
    "            val_e=val_embeddings, val_pred=val_predictions, val_y=val_y, val_m=val_metadata,\n",
    "            w0 = classifier.weight.cpu(),\n",
    "            b0 = classifier.bias.cpu()\n",
    "        ),\n",
    "        save_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = '/home/andres_p/gdro_fork/logs/multinli' # TODO: change to your directory of checkpoints\n",
    "ckpts = ['erm_dfrdrop0', 'erm_dfrdrop_1', 'erm_dfrdrop2'] # TODO: change to your checkpoints\n",
    "seeds = [0, 1, 2] # TODO: change to your seeds for splitting the train data\n",
    "for ckpt, seed in zip(ckpts, seeds):\n",
    "    ckpt_path = f'{ckpt_dir}/{ckpt}/last_model.pth'\n",
    "    save_path = f'../emb/multinli/{ckpt}.pt'\n",
    "    save_emb(ckpt_path, seed, save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AFR ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Modify these to your own paths ###\n",
    "emb_paths = [\n",
    "    '../emb/multinli/erm_dfrdrop0.pt',\n",
    "    '../emb/multinli/erm_dfrdrop1.pt',\n",
    "    '../emb/multinli/erm_dfrdrop2.pt',\n",
    "]\n",
    "### The remaining code needs zero modification ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "\n",
    "# import from parent directory\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "import models\n",
    "import utils\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(emb_path):\n",
    "    trian_group_ratios = np.array([57498, 11158, 67376, 1521, 66630, 1992]).astype(float)\n",
    "    trian_group_ratios = trian_group_ratios / trian_group_ratios.sum()\n",
    "    print(f'Loading embeddings from {emb_path}')\n",
    "    emb_dict = torch.load(emb_path)\n",
    "    reweighting_embeddings, reweighting_predictions, reweighting_y, reweighting_groups = emb_dict['e'], emb_dict['pred'], emb_dict['y'], emb_dict['m']\n",
    "    test_embeddings, test_predictions, test_y, test_groups = emb_dict['test_e'], emb_dict['test_pred'], emb_dict['test_y'], emb_dict['test_m']\n",
    "    val_embeddings, val_predictions, val_y, val_groups = emb_dict['val_e'], emb_dict['val_pred'], emb_dict['val_y'], emb_dict['val_m']\n",
    "\n",
    "    group_counts = {}\n",
    "    for g in reweighting_groups:\n",
    "            group_counts[g.item()] = group_counts.get(g.item(), 0) + 1\n",
    "    print('*** Reweighting Counts + Group distribution: ***')\n",
    "    for g in sorted(group_counts.keys()):\n",
    "        print(f'Group {g}: {group_counts[g]} ({(group_counts[g] / sum(group_counts.values()) * 100):.1f}%)')\n",
    "\n",
    "    print('*** Reweighting Acc + Group distribution: ***')        \n",
    "    print_accs(reweighting_predictions, reweighting_y, reweighting_groups)\n",
    "\n",
    "    print('*** Test Acc + Group distribution: ***')        \n",
    "    print_accs(test_predictions, test_y, test_groups)\n",
    "\n",
    "    print('*** Val Acc + Group distribution: ***')\n",
    "    print_accs(val_predictions, val_y, val_groups)\n",
    "    emb_dict['trian_group_ratios'] = trian_group_ratios\n",
    "    emb_dict['g'] = emb_dict['m']\n",
    "    emb_dict['test_g'] = emb_dict['test_m']\n",
    "    emb_dict['val_g'] = emb_dict['val_m']\n",
    "    return emb_dict\n",
    "\n",
    "def get_accs(test_predictions, test_y, test_groups):\n",
    "    return [(test_predictions == test_y)[test_groups == g].float().mean().item() for g in torch.unique(test_groups)]\n",
    "\n",
    "def print_accs(test_predictions, test_y, test_groups):\n",
    "    acc = (test_predictions == test_y).float().mean()\n",
    "    group_accs = get_accs(test_predictions, test_y, test_groups)\n",
    "    group_counts = {}\n",
    "    for g in test_groups:\n",
    "            group_counts[g.item()] = group_counts.get(g.item(), 0) + 1\n",
    "    print(f\"Avg: {acc:.3f}\")\n",
    "    for g, acc in enumerate(group_accs):\n",
    "        print(f\"Group {g}: {acc:.2f} ({(group_counts[g] / sum(group_counts.values()) * 100):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wxe_fn(logits, y, weights):\n",
    "    ce = torch.nn.functional.cross_entropy(logits, y, reduction='none')\n",
    "    l = weights * ce\n",
    "    return l.sum()\n",
    "\n",
    "def compute_afr_weights(erm_logits, class_label, gamma, balance_classes):\n",
    "    # erm_logits: (n_samples, n_classes)\n",
    "    # class_label: (n_samples,)\n",
    "    # gamma: float\n",
    "    with torch.no_grad():\n",
    "        p = erm_logits.softmax(-1)\n",
    "        y_onehot = torch.zeros_like(erm_logits).scatter_(-1, class_label.unsqueeze(-1), 1)\n",
    "        p_true = (p * y_onehot).sum(-1)\n",
    "        weights = (-gamma * p_true).exp()\n",
    "        n_classes = torch.unique(class_label).numel()\n",
    "        # class balancing\n",
    "        if balance_classes:\n",
    "            class_count = []\n",
    "            for y in range(n_classes):\n",
    "                class_count.append((class_label == y).sum())\n",
    "            for y in range(1, n_classes):\n",
    "                weights[class_label == y] *= class_count[0] / class_count[y]\n",
    "        weights /= weights.sum()\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(emb_dict, num_epochs, gamma, reg_coeff, lr=1e-2, balance_classes=False, early_stop='wga', plot=True, silent=False, group_uniform=False, opt='sgd'):\n",
    "    reweighting_embeddings = emb_dict['e'].cuda()\n",
    "    reweighting_y = emb_dict['y'].cuda()\n",
    "    reweighting_groups = emb_dict['g'].cuda()\n",
    "    test_embeddings = emb_dict['test_e'].cuda()\n",
    "    test_y = emb_dict['test_y'].cuda()\n",
    "    test_groups = emb_dict['test_g'].cuda()\n",
    "    val_embeddings = emb_dict['val_e'].cuda()\n",
    "    val_y = emb_dict['val_y'].cuda()\n",
    "    val_groups = emb_dict['val_g'].cuda()\n",
    "    w0 = emb_dict['w0'].cuda()\n",
    "    b0 = emb_dict['b0'].cuda()\n",
    "    trian_group_ratios = emb_dict['trian_group_ratios']\n",
    "    num_groups = torch.unique(test_groups).numel()\n",
    "\n",
    "    class Model(torch.nn.Module):\n",
    "        def __init__(self, w0, b0):\n",
    "            super().__init__()\n",
    "            self.w0 = w0\n",
    "            self.b0 = b0\n",
    "            self.linear = torch.nn.Linear(w0.shape[1], w0.shape[0], bias=True)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            y_old = x @ self.w0.t() + self.b0\n",
    "            y_new = self.linear(x)\n",
    "            return y_old + y_new\n",
    "\n",
    "    model = Model(w0, b0)\n",
    "    model.cuda()\n",
    "\n",
    "    criterion = wxe_fn\n",
    "    if opt == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0., momentum=0.)\n",
    "    elif opt == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    mean_accs = []\n",
    "    test_mean_accs = []\n",
    "    mean_group_accs = []\n",
    "    val_mean_group_accs = []\n",
    "    wgas = []\n",
    "    test_wgas = []\n",
    "    val_wgas = []\n",
    "    losses = []\n",
    "    regs = []\n",
    "    weights_by_groups = []\n",
    "    acc_by_groups = []\n",
    "    val_acc_by_groups = []\n",
    "    test_accs_by_groups = []\n",
    "    weighted_acc = []\n",
    "    val_weighted_acc = []\n",
    "\n",
    "    initial_logits = reweighting_embeddings @ w0.t() + b0\n",
    "    initial_val_logits = val_embeddings @ w0.t() + b0\n",
    "    weights = compute_afr_weights(initial_logits, reweighting_y, gamma, balance_classes)\n",
    "    val_weights = compute_afr_weights(initial_val_logits, val_y, gamma, balance_classes)\n",
    "    if group_uniform:\n",
    "        # uniform total weights per group\n",
    "        group_counts = torch.bincount(reweighting_groups)\n",
    "        for g in range(len(group_counts)):\n",
    "            weights[reweighting_groups == g] = 1 / group_counts[g] / len(group_counts)\n",
    "    for _ in (pbar := tqdm.tqdm(range(num_epochs), disable=silent)):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(reweighting_embeddings)\n",
    "        loss = criterion(logits, reweighting_y, weights)\n",
    "        reg = model.linear.weight.pow(2).sum() + model.linear.bias.pow(2).sum()\n",
    "        loss += reg_coeff * reg\n",
    "        # additional infos\n",
    "        with torch.no_grad():\n",
    "            val_logits = model(val_embeddings)\n",
    "            ce = torch.nn.functional.cross_entropy(logits, reweighting_y, reduction='none')\n",
    "            accs = get_accs(torch.argmax(logits, -1), reweighting_y, reweighting_groups)\n",
    "            val_accs = get_accs(torch.argmax(model(val_embeddings), -1), val_y, val_groups)\n",
    "            test_accs = get_accs(torch.argmax(model(test_embeddings), -1), test_y, test_groups)\n",
    "\n",
    "        loss.backward()\n",
    "        # clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "        optimizer.step()\n",
    "\n",
    "        weights_by_group = [weights[reweighting_groups == g].sum().item() for g in range(num_groups)]\n",
    "        weights_by_groups.append(weights_by_group)\n",
    "        acc_by_groups.append(accs)\n",
    "        val_acc_by_groups.append(val_accs)\n",
    "        test_accs_by_groups.append(test_accs)\n",
    "        mean_acc = (np.array(accs) * np.array(trian_group_ratios)).sum()\n",
    "        mean_group_accs.append(np.array(accs).mean())\n",
    "        mean_accs.append(mean_acc)\n",
    "        val_mean_group_accs.append(np.array(val_accs).mean())\n",
    "        wga = min(accs)\n",
    "        test_wga = min(test_accs)\n",
    "        val_wga = min(val_accs)\n",
    "        wgas.append(wga)\n",
    "        val_wgas.append(val_wga)\n",
    "        test_wgas.append(test_wga)\n",
    "        # test mean acc is test_accs weighted by trian_group_ratios\n",
    "        test_mean_acc = (np.array(test_accs) * np.array(trian_group_ratios)).sum()\n",
    "        test_mean_accs.append(test_mean_acc)\n",
    "        losses.append(loss.item())\n",
    "        regs.append(reg.item())\n",
    "        weighted_acc.append((((torch.argmax(logits, -1) == reweighting_y).float() * weights).sum() / weights.sum()).item())\n",
    "        val_weighted_acc.append((((torch.argmax(val_logits, -1) == val_y).float() * val_weights).sum() / val_weights.sum()).item())\n",
    "        pbar.set_description(f\"Loss: {loss.item():.5f}, WGA: {wga:.3f}, TWGA: {test_wga:.3f}\")\n",
    "\n",
    "    \n",
    "    if early_stop == 'wga':\n",
    "        earlystop_epoch = np.argmax(val_wgas)\n",
    "        early_stop_metric = np.max(val_wgas)\n",
    "    elif early_stop == 'wga_last':\n",
    "        # take best wga on validation set\n",
    "        # if multiple epochs have the same wga, take the last one\n",
    "        earlystop_epoch = argmax_last(val_wgas)\n",
    "        early_stop_metric = np.max(val_wgas)\n",
    "    elif early_stop == 'wga@max_val_wa':\n",
    "        earlystop_epoch = np.argmax(val_weighted_acc)\n",
    "        early_stop_metric = val_wgas[earlystop_epoch]\n",
    "    elif early_stop == 'mga':\n",
    "        earlystop_epoch = np.argmax(val_mean_group_accs)\n",
    "        early_stop_metric = np.max(val_mean_group_accs)\n",
    "    elif early_stop == 'none':\n",
    "        earlystop_epoch = num_epochs - 1\n",
    "        early_stop_metric = val_wgas[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown early_stop: {early_stop}\")\n",
    "    earlystop_twga = test_wgas[earlystop_epoch]\n",
    "    earlystop_tmacc = test_mean_accs[earlystop_epoch]\n",
    "    final_twga = test_wgas[-1]\n",
    "    final_tmacc = test_mean_accs[-1]\n",
    "    if plot:\n",
    "        # print test wga at best reweighting wga epoch\n",
    "        print(f\"Final test WGA: {final_twga:.3f}\")\n",
    "        print(f\"Final test mean acc: {final_tmacc:.3f}\")\n",
    "        print(f\"Early stopping test WGA: {earlystop_twga:.3f} at epoch {earlystop_epoch}\") \n",
    "        print(f\"Early stopping test mean acc: {earlystop_tmacc:.3f} at epoch {earlystop_epoch}\")\n",
    "\n",
    "        # 3 horizontal subplots\n",
    "        fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20, 5))\n",
    "        ax1.plot(losses, label=\"CWXE\")\n",
    "        ax1.plot(regs, label=\"Reg\")\n",
    "        ax1.grid()\n",
    "        ax1.legend(loc=\"lower right\")\n",
    "        ax1.set_xlabel(\"Epochs\")\n",
    "        ax1.set_ylabel(\"Loss\")\n",
    "\n",
    "        ax2.plot(test_wgas, label=\"Test\")\n",
    "        ax2.plot(wgas, label=\"Reweighting\")\n",
    "        ax2.plot(val_wgas, label=\"Validation\")\n",
    "        ax2.grid()\n",
    "        ax2.legend(loc=\"lower right\")\n",
    "        ax2.set_xlabel(\"Epochs\")\n",
    "        ax2.set_ylabel(\"WGA\")\n",
    "\n",
    "        ax3.plot(test_mean_accs, label=\"Test\")\n",
    "        ax3.plot(mean_accs, label=\"Reweighting\")\n",
    "        ax3.grid()\n",
    "        ax3.legend(loc=\"lower right\")\n",
    "        ax3.set_xlabel(\"Epochs\")\n",
    "        ax3.set_ylabel(\"Mean Acc\")\n",
    "\n",
    "        ax4.plot(weighted_acc, label=\"Reweighting\")\n",
    "        ax4.plot(val_weighted_acc, label=\"Validation\")\n",
    "        ax4.grid()\n",
    "        ax4.set_xlabel(\"Epochs\")\n",
    "        ax4.set_ylabel(\"Weighted Acc\")\n",
    "        ax4.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "        # 3 horizontal subplots\n",
    "        fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "        ax1.plot(weights_by_groups, label=[f'G{g+1}' for g in range(num_groups)])\n",
    "        ax1.grid()\n",
    "        ax1.set_xlabel(\"Epochs\")\n",
    "        ax1.set_ylabel(\"Weights\")\n",
    "        ax1.legend(loc=\"lower right\")\n",
    "\n",
    "        for g in range(num_groups):\n",
    "            ax2.plot([acc[g] for acc in acc_by_groups], label=f\"G{g+1}\", color=f\"C{g}\")\n",
    "            # ax2.plot([acc[g] for acc in val_acc_by_groups], color=f\"C{g}\", linestyle=\"--\")\n",
    "            ax2.plot([acc[g] for acc in test_accs_by_groups], color=f\"C{g}\", linestyle=\":\")\n",
    "        ax2.grid()\n",
    "        ax2.set_xlabel(\"Epochs\")\n",
    "        ax2.set_ylabel(\"Acc\")\n",
    "        ax2.legend(loc=\"lower right\")\n",
    "\n",
    "    return early_stop_metric, earlystop_twga, earlystop_tmacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_sampler(sweep_config, random=True):\n",
    "    if random:\n",
    "        while True:\n",
    "            gamma = np.random.choice(sweep_config[\"gamma_choices\"])\n",
    "            reg_coeff = np.random.choice(sweep_config[\"reg_coeff_choices\"])\n",
    "            balance_classes = np.random.choice(sweep_config[\"balance_classes_choices\"])\n",
    "            yield gamma, reg_coeff, balance_classes\n",
    "    else:\n",
    "        for gamma in sweep_config[\"gamma_choices\"]:\n",
    "            for reg_coeff in sweep_config[\"reg_coeff_choices\"]:\n",
    "                for balance_classes in sweep_config[\"balance_classes_choices\"]:\n",
    "                    yield gamma, reg_coeff, balance_classes\n",
    "\n",
    "def run_training(emb_dict, num_epochs, lr, gamma, reg_coeff, balance_classes, early_stop):\n",
    "    return train(emb_dict, num_epochs=num_epochs, gamma=gamma, reg_coeff=reg_coeff, lr=lr, balance_classes=balance_classes, early_stop=early_stop, plot=False, silent=True), (gamma, reg_coeff, balance_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep(ckpts, sweep_config, skip_if_done, seed=0):\n",
    "    best_twgas = []\n",
    "    best_tmaccs = []\n",
    "    for ckpt in ckpts:\n",
    "        emb_dict = extract_embeddings(ckpt)\n",
    "        sweep_file = os.path.join(os.path.dirname(ckpt), f'seed{ckpt[-4]}_' + sweep_config['sweep_name'] + '.pt')\n",
    "\n",
    "        if skip_if_done and os.path.exists(sweep_file):\n",
    "            twga = torch.load(sweep_file)['best_twga']\n",
    "            print(f\"Skipping {ckpt}: TWGA {twga:.3f}\")\n",
    "            best_twgas.append(twga)\n",
    "            best_tmaccs.append(torch.load(sweep_file)['best_tmacc'])\n",
    "            continue\n",
    "\n",
    "        # break\n",
    "        # find best hyperparameters\n",
    "        best_vwga = 0\n",
    "        best_twga = 0\n",
    "        best_tmacc = 0\n",
    "        best_hyperparams = None\n",
    "        top10_choices = []\n",
    "        \n",
    "        # fix all seed\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        import random\n",
    "        random.seed(seed)\n",
    "        \n",
    "        # subsample validation set\n",
    "        if sweep_config['val_prop'] < 1:\n",
    "            val_size = int(emb_dict['val_e'].shape[0] * sweep_config['val_prop'])\n",
    "            val_idx = np.random.choice(emb_dict['val_e'].shape[0], val_size, replace=False)\n",
    "            emb_dict['val_e'] = emb_dict['val_e'][val_idx]\n",
    "            emb_dict['val_y'] = emb_dict['val_y'][val_idx]\n",
    "            emb_dict['val_g'] = emb_dict['val_g'][val_idx]\n",
    "        print(f'Tuning hypers on {emb_dict[\"val_e\"].shape[0]} val examples')\n",
    "        sampler = hyper_sampler(sweep_config, random=sweep_config['random'])\n",
    "        for i in (pbar:= tqdm.tqdm(range(sweep_config['n_trials']))):\n",
    "            try:\n",
    "                gamma, reg_coeff, balance_classes = next(sampler)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            (val_wga, test_wga, test_macc, *_), hyperparams = run_training(emb_dict, sweep_config['num_epochs'], sweep_config['lr'], gamma, reg_coeff, balance_classes, sweep_config['early_stop'])\n",
    "            if val_wga > best_vwga:\n",
    "                best_vwga = val_wga\n",
    "                best_twga = test_wga\n",
    "                best_tmacc = test_macc\n",
    "                best_hyperparams = hyperparams\n",
    "                gamma, reg_coeff, balance_classes = best_hyperparams\n",
    "                pbar.set_description(f\"Test WGA: {test_wga:.3f}, Val WGA: {val_wga:.3f}, gamma: {gamma:.3f}, reg_coeff: {reg_coeff:.3f}, balance_classes: {balance_classes}\")\n",
    "                if val_wga == 1:\n",
    "                    break\n",
    "            if len(top10_choices) < 10:\n",
    "                top10_choices.append((test_wga, val_wga, hyperparams))\n",
    "                top10_choices = sorted(top10_choices, key=lambda x: x[1], reverse=True)\n",
    "            else:\n",
    "                top10_choices = sorted(top10_choices, key=lambda x: x[1], reverse=True)\n",
    "                if val_wga > top10_choices[-1][1]:\n",
    "                    top10_choices[-1] = (test_wga, val_wga, hyperparams)\n",
    "\n",
    "        # save sweep results, indicate data_transform and num aug\n",
    "        torch.save({\n",
    "            'best_twga': best_twga,\n",
    "            'best_tmacc': best_tmacc,\n",
    "            'best_hyperparams': best_hyperparams,\n",
    "            'top10_choices': top10_choices,\n",
    "            'sweep_config': sweep_config,\n",
    "        }, sweep_file)\n",
    "        best_twgas.append(best_twga)\n",
    "        best_tmaccs.append(best_tmacc)\n",
    "\n",
    "    # print mean and std of best WGA\n",
    "    print(f\"Mean best WGA: {np.mean(best_twgas):.3f}, std: {np.std(best_twgas):.3f}\")\n",
    "    # print mean and std of mean acc\n",
    "    print(f\"Mean best mean acc: {np.mean(best_tmaccs):.3f}, std: {np.std(best_tmaccs):.3f}\")\n",
    "    return np.mean(best_twgas), np.std(best_twgas), np.mean(best_tmaccs), np.std(best_tmaccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_prop = 1\n",
    "skip_if_done = True\n",
    "\n",
    "sweep_config = {\n",
    "    \"n_trials\": 260,\n",
    "    \"num_epochs\": 200,\n",
    "    \"lr\": 1e-2,\n",
    "    \"gamma_choices\": np.logspace(2, 5, 10),\n",
    "    \"reg_coeff_choices\": np.linspace(0, 50, 26),\n",
    "    \"balance_classes_choices\": [True],\n",
    "    \"early_stop\": 'wga',\n",
    "    \"random\": False, # if True then random search, else grid search\n",
    "    \"val_prop\": val_prop,\n",
    "    \"sweep_name\": 'nominal',\n",
    "    }\n",
    "sweep(emb_paths, sweep_config, skip_if_done)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Label Efficiency / Down-sampled Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_if_done = True\n",
    "\n",
    "for seed in [0, 21, 42]:\n",
    "    for val_prop in [0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1]:\n",
    "        sweep_config = {\n",
    "            \"n_trials\": 260,\n",
    "            \"num_epochs\": 500,\n",
    "            \"lr\": 1e-2,\n",
    "            \"gamma_choices\": np.logspace(2, 5, 10),\n",
    "            \"reg_coeff_choices\": np.linspace(0, 50, 26),\n",
    "            \"balance_classes_choices\": [True],\n",
    "            \"early_stop\": 'wga',\n",
    "            \"random\": False, # if True then random search, else grid search\n",
    "            \"val_prop\": val_prop,\n",
    "            'sweep_name': f'val_prop_{val_prop}_seed{seed}'\n",
    "            }\n",
    "        sweep(emb_paths, sweep_config, skip_if_done, seed=seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
