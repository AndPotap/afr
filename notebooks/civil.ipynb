{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env OMP_NUM_THREADS=4\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Modify to your own data dir ###\n",
    "data_dir = '/data/users/pavel_i/datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../wilds_exps_utils/\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import pickle\n",
    "import copy\n",
    "from types import SimpleNamespace\n",
    "from wilds import get_dataset\n",
    "from wilds.common.data_loaders import get_eval_loader\n",
    "from wilds_configs import datasets as dataset_configs\n",
    "from wilds.datasets.wilds_dataset import WILDSSubset\n",
    "from wilds_models.initializer import initialize_model\n",
    "import wilds_transforms as transforms\n",
    "\n",
    "# from wilds_algorithms.initializer import infer_d_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'civilcomments'\n",
    "\n",
    "def get_data(args, wilds_config):\n",
    "    full_dataset = get_dataset(dataset=DATASET, download=False, root_dir=args.root_dir)\n",
    "    transform = transforms.initialize_transform(\n",
    "            transform_name=wilds_config.transform,\n",
    "            config=wilds_config,\n",
    "            dataset=full_dataset,\n",
    "            additional_transform_name=None,\n",
    "            is_training=False)\n",
    "\n",
    "    test_data = full_dataset.get_subset(\"test\", transform=transform)\n",
    "    val_data = full_dataset.get_subset(\"val\", transform=transform)\n",
    "\n",
    "    if args.dfr_reweighting_drop:\n",
    "        train_data = full_dataset.get_subset(\"train\", transform=transform)\n",
    "        idx = train_data.indices.copy()\n",
    "        rng = np.random.default_rng(args.dfr_reweighting_seed)\n",
    "        rng.shuffle(idx)\n",
    "        n_train = int((1 - args.dfr_reweighting_frac) * len(idx))\n",
    "        val_idx = idx[n_train:]\n",
    "        reweighting_data = WILDSSubset(\n",
    "                full_dataset,\n",
    "                indices=val_idx,\n",
    "                transform=transform)\n",
    "        del train_data\n",
    "    else:\n",
    "        reweighting_data = val_data\n",
    "    return val_data, test_data, reweighting_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just need the ds object for evaluation\n",
    "DATASET = 'civilcomments'\n",
    "seed = 2\n",
    "args = SimpleNamespace(\n",
    "    root_dir=data_dir,\n",
    "    batch_size=16,\n",
    "    dfr_reweighting_drop=True,\n",
    "    dfr_reweighting_seed=seed,\n",
    "    dfr_reweighting_frac=0.2,\n",
    ")\n",
    "wilds_config = SimpleNamespace(\n",
    "    algorithm='ERM',\n",
    "    load_featurizer_only=False,\n",
    "    pretrained_model_path=None,\n",
    "    **dataset_configs.dataset_defaults[DATASET],\n",
    ")\n",
    "wilds_config.model_kwargs = {}\n",
    "wilds_config.model = 'bert-base-uncased'\n",
    "\n",
    "val_data, test_data, reweighting_data = get_data(args, wilds_config)\n",
    "reweighting_loader = get_eval_loader(\"standard\", reweighting_data, batch_size=args.batch_size)\n",
    "val_loader = get_eval_loader(\"standard\", val_data, batch_size=args.batch_size)\n",
    "test_loader = get_eval_loader(\"standard\", test_data, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(args, wilds_config, d_out):\n",
    "    model = initialize_model(config=wilds_config, d_out=d_out)\n",
    "    ckpt_dict = torch.load(args.ckpt_path)\n",
    "    model.load_state_dict({k[len('model.'):]: v for (k, v) in ckpt_dict['algorithm'].items()})\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    classifier = model.classifier\n",
    "    model.classifier = torch.nn.Identity(model.classifier.in_features)\n",
    "    return model, classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_predictions(feature_extractor, classifier, loader):\n",
    "    all_embeddings, all_predictions, all_y_true, all_metadata = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y_true, metadata in tqdm.tqdm(loader):\n",
    "            embeddings = feature_extractor(x.cuda())\n",
    "            predictions = torch.argmax(classifier(embeddings), axis=1)\n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "            all_predictions.append(predictions.cpu())\n",
    "            all_y_true.append(y_true.cpu())\n",
    "            all_metadata.append(metadata)\n",
    "    all_embeddings = torch.cat(all_embeddings, axis=0)\n",
    "    all_predictions = torch.cat(all_predictions, axis=0)\n",
    "    all_y_true = torch.cat(all_y_true, axis=0)\n",
    "    all_metadata = torch.cat(all_metadata, axis=0)\n",
    "    return all_embeddings, all_predictions, all_y_true, all_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_emb(ckpt_path, seed, save_path):\n",
    "        args = SimpleNamespace(\n",
    "        root_dir=data_dir,\n",
    "        batch_size=16,\n",
    "        dfr_reweighting_drop=True,\n",
    "        dfr_reweighting_seed=seed,\n",
    "        dfr_reweighting_frac=0.2,\n",
    "        ckpt_path=ckpt_path\n",
    "        )\n",
    "        wilds_config = SimpleNamespace(\n",
    "        algorithm='ERM',\n",
    "        load_featurizer_only=False,\n",
    "        pretrained_model_path=None,\n",
    "        **dataset_configs.dataset_defaults[DATASET],\n",
    "        )\n",
    "        wilds_config.model_kwargs = {}\n",
    "        wilds_config.model = 'bert-base-uncased'\n",
    "\n",
    "        val_data, test_data, reweighting_data = get_data(args, wilds_config)\n",
    "        bad_test_idx = 73529 # this example is somehow broken ...\n",
    "        test_data.indices = np.delete(test_data.indices, bad_test_idx)\n",
    "        reweighting_loader = get_eval_loader(\"standard\", reweighting_data, batch_size=args.batch_size)\n",
    "        val_loader = get_eval_loader(\"standard\", val_data, batch_size=args.batch_size)\n",
    "        test_loader = get_eval_loader(\"standard\", test_data, batch_size=args.batch_size)\n",
    "\n",
    "        d_out = 2\n",
    "        feature_extractor, classifier = load_model(args, wilds_config, d_out)\n",
    "\n",
    "        reweighting_embeddings, reweighting_predictions, reweighting_y, reweighting_metadata = get_embeddings_predictions(\n",
    "                feature_extractor, classifier, reweighting_loader)\n",
    "        val_embeddings, val_predictions, val_y, val_metadata = get_embeddings_predictions(\n",
    "                feature_extractor, classifier, val_loader)\n",
    "        test_embeddings, test_predictions, test_y, test_metadata = get_embeddings_predictions(\n",
    "                feature_extractor, classifier, test_loader)\n",
    "        torch.save(\n",
    "        dict(\n",
    "                e=reweighting_embeddings, y=reweighting_y, pred=reweighting_predictions, m=reweighting_metadata,\n",
    "                test_e=test_embeddings, test_pred=test_predictions, test_y=test_y, test_m=test_metadata,\n",
    "                val_e=val_embeddings, val_pred=val_predictions, val_y=val_y, val_m=val_metadata,\n",
    "                w0 = classifier.weight.cpu(),\n",
    "                b0 = classifier.bias.cpu()\n",
    "        ),\n",
    "        save_path\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "ckpt_dir = '/data/users/pavel_i/wilds_ckpts' # TODO: change to your ckpt dir\n",
    "seeds = [0, 1, 2] # TODO: change to your seeds for splitting the train data\n",
    "save_dir = '../emb/civil'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "for seed in seeds:\n",
    "    ckpt_path = glob(f'{ckpt_dir}/bert_civilcomments_dfrdrop_{seed}/*last_model.pth') # TODO: change to your ckpt path\n",
    "    assert len(ckpt_path) == 1, f'Found {len(ckpt_path)} ckpts for seed {seed}, expected 1'\n",
    "    ckpt_path = ckpt_path[0]\n",
    "    save_path = f'{save_dir}/{seed}.pt'\n",
    "    save_emb(ckpt_path, seed, save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(metrics):\n",
    "    print(\"Early stopping by Val\")\n",
    "    earlystop_epoch = np.argmax(np.array(metrics.val_accs[\"wga\"]))\n",
    "    print(f\"test WGA: {metrics.test_accs['wga'][earlystop_epoch]:.3f} at epoch {earlystop_epoch}\") \n",
    "    # print(f\"Early stopping test mean acc: {metrics.test_accs['mean_acc'][earlystop_epoch]:.3f} at epoch {earlystop_epoch}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Early stopping by Val\")\n",
    "    earlystop_epoch = np.argmax(metrics.val_accs[\"combined_wga\"])\n",
    "    print(f\"test WGA: {metrics.test_accs['wga'][earlystop_epoch]:.3f} at epoch {earlystop_epoch}\") \n",
    "    # print(f\"Early stopping test mean acc: {metrics.test_accs['mean_acc'][earlystop_epoch]:.3f} at epoch {earlystop_epoch}\")\n",
    "    print()\n",
    "    \n",
    "    # 3 horizontal subplots\n",
    "    fig, arr = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    arr[0].plot(metrics.losses, label=\"wxe\")\n",
    "    arr[0].plot(metrics.regs, label=\"Reg\")\n",
    "    arr[0].grid()\n",
    "    arr[0].legend(loc=\"lower right\")\n",
    "    arr[0].set_xlabel(\"Epochs\")\n",
    "    arr[0].set_ylabel(\"Loss\")\n",
    "\n",
    "    arr[1].plot(metrics.test_accs[\"wga\"], label=\"Test\")\n",
    "    arr[1].plot(metrics.reweighting_accs[\"wga\"], label=\"Reweighting\")\n",
    "    arr[1].plot(metrics.val_accs[\"wga\"], label=\"Validation\")\n",
    "    arr[1].grid()\n",
    "    arr[1].legend(loc=\"lower right\")\n",
    "    arr[1].set_xlabel(\"Epochs\")\n",
    "    arr[1].set_ylabel(\"WGA\")\n",
    "    arr[1].set_ylim(0.5, 0.75)\n",
    "\n",
    "    arr[2].plot(metrics.test_accs[\"mean_acc\"], label=\"Test\")\n",
    "    arr[2].plot(metrics.reweighting_accs[\"mean_acc\"], label=\"Reweighting\")\n",
    "    arr[2].plot(metrics.val_accs[\"mean_acc\"], label=\"Validation\")\n",
    "    arr[2].grid()\n",
    "    arr[2].legend(loc=\"lower right\")\n",
    "    arr[2].set_xlabel(\"Epochs\")\n",
    "    arr[2].set_ylabel(\"Mean Acc\")\n",
    "\n",
    "    # 4 horizontal subplots\n",
    "    fig, arr = plt.subplots(2, 3, figsize=(20, 10))\n",
    "\n",
    "    colors = ['r', 'g', 'b', 'y', 'm', 'c', 'k', 'orange']\n",
    "    for attr, c in enumerate(colors):\n",
    "        arr[0, 0].plot([ls[attr] for ls in metrics.loss_by_groups['toxic']], color=c)\n",
    "        arr[0, 0].plot([ls[attr] for ls in metrics.loss_by_groups['non_toxic']], '--', color=c)\n",
    "    arr[0, 0].grid()\n",
    "    arr[0, 0].set_xlabel(\"Epochs\")\n",
    "    arr[0, 0].set_ylabel(\"Loss\")\n",
    "    arr[0, 0].legend(loc=\"lower right\")\n",
    "\n",
    "    for attr, c in enumerate(colors):\n",
    "        arr[0, 1].plot([ls[attr] for ls in metrics.weights_by_groups['toxic']], color=c)\n",
    "        arr[0, 1].plot([ls[attr] for ls in metrics.weights_by_groups['non_toxic']], '--', color=c)\n",
    "    arr[0, 1].grid()\n",
    "    arr[0, 1].set_xlabel(\"Epochs\")\n",
    "    arr[0, 1].set_ylabel(\"Weights\")\n",
    "    arr[0, 1].legend(loc=\"lower right\")\n",
    "\n",
    "    for attr, c in enumerate(colors):\n",
    "        arr[0, 2].plot([ls[attr] for ls in metrics.confidence_by_groups['toxic']], color=c)\n",
    "        arr[0, 2].plot([ls[attr] for ls in metrics.confidence_by_groups['non_toxic']], '--', color=c)\n",
    "    arr[0, 2].grid()\n",
    "    arr[0, 2].set_xlabel(\"Epochs\")\n",
    "    arr[0, 2].set_ylabel(\"Confidence\")\n",
    "    arr[0, 2].legend(loc=\"lower right\")\n",
    "\n",
    "    for attr, c in enumerate(colors):\n",
    "        arr[1, 0].plot([ls[attr] for ls in metrics.test_accs['by_group_toxic']], color=c)\n",
    "        arr[1, 0].plot([ls[attr] for ls in metrics.test_accs['by_group_nontoxic']], '--', color=c)\n",
    "    arr[1, 0].plot(metrics.test_accs['combined_wga'], '-*', color='k')\n",
    "    arr[1, 0].grid()\n",
    "    arr[1, 0].set_xlabel(\"Epochs\")\n",
    "    arr[1, 0].set_ylabel(\"Test Acc\")\n",
    "    arr[1, 0].legend(loc=\"lower right\")\n",
    "    \n",
    "    for attr, c in enumerate(colors):\n",
    "        arr[1, 1].plot([ls[attr] for ls in metrics.val_accs['by_group_toxic']], color=c)\n",
    "        arr[1, 1].plot([ls[attr] for ls in metrics.val_accs['by_group_nontoxic']], '--', color=c)\n",
    "    arr[1, 1].plot(metrics.val_accs['combined_wga'], '-*', color='k')\n",
    "    arr[1, 1].grid()\n",
    "    arr[1, 1].set_xlabel(\"Epochs\")\n",
    "    arr[1, 1].set_ylabel(\"Val Acc\")\n",
    "    arr[1, 1].legend(loc=\"lower right\")\n",
    "    \n",
    "    for attr, c in enumerate(colors):\n",
    "        arr[1, 2].plot([ls[attr] for ls in metrics.reweighting_accs['by_group_toxic']], color=c)\n",
    "        arr[1, 2].plot([ls[attr] for ls in metrics.reweighting_accs['by_group_nontoxic']], '--', color=c)\n",
    "    arr[1, 2].plot(metrics.reweighting_accs['combined_wga'], '-*', color='k')\n",
    "    arr[1, 2].grid()\n",
    "    arr[1, 2].set_xlabel(\"Epochs\")\n",
    "    arr[1, 2].set_ylabel(\"Reweighting Acc\")\n",
    "    arr[1, 2].legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "    def __init__(\n",
    "            self, reweighting_metadata, test_metadata, val_metadata):\n",
    "        \n",
    "        self.n_cols = 8\n",
    "        \n",
    "        self.reweighting_metadata = reweighting_metadata\n",
    "        self.test_metadata = test_metadata\n",
    "        self.val_metadata = val_metadata\n",
    "        \n",
    "        self.reweighting_accs = self.get_init_accs()\n",
    "        self.test_accs = self.get_init_accs()\n",
    "        self.val_accs = self.get_init_accs()\n",
    "        \n",
    "        self.weights_by_groups = self.get_init_dict()\n",
    "        self.loss_by_groups = self.get_init_dict()\n",
    "        self.confidence_by_groups = self.get_init_dict()\n",
    "        \n",
    "        keys = (\"toxic\", \"non_toxic\")\n",
    "        self.reweighting_group_sizes = dict(zip(keys, self._something_by_groups(\n",
    "                torch.ones_like(reweighting_metadata[:, 0]), reweighting_metadata)))\n",
    "        self.val_group_sizes = dict(zip(keys, self._something_by_groups(\n",
    "                torch.ones_like(val_metadata[:, 0]), val_metadata)))\n",
    "        self.test_group_sizes = dict(zip(keys, self._something_by_groups(\n",
    "                torch.ones_like(test_metadata[:, 0]), test_metadata)))\n",
    "        \n",
    "        self.losses = []\n",
    "        self.regs = []\n",
    "        \n",
    "#         self.weighted_acc = []\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_init_dict():\n",
    "        return {'toxic': [], 'non_toxic': []}\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_init_accs():\n",
    "        return {\n",
    "            'mean_acc': [],\n",
    "            'wga': [],\n",
    "            'combined_wga': [],\n",
    "            'by_group_toxic': [],\n",
    "            'by_group_nontoxic': []\n",
    "        }\n",
    "    \n",
    "    def _something_by_groups(self, something, metadata, mean=False):\n",
    "        if mean: \n",
    "            return (\n",
    "                [something[(metadata[:, i] == 1) & (metadata[:, -2] == v)].float().mean().item() for i in range(self.n_cols)]\n",
    "                for v in [0, 1])\n",
    "        else:\n",
    "            return (\n",
    "                [something[(metadata[:, i] == 1) & (metadata[:, -2] == v)].sum().item() for i in range(self.n_cols)]\n",
    "                for v in [0, 1])\n",
    "    \n",
    "    def _update_weights_by_groups(self, weights):\n",
    "        toxic, nontoxic = self._something_by_groups(\n",
    "            weights, self.reweighting_metadata)\n",
    "        self.weights_by_groups['toxic'].append(toxic)\n",
    "        self.weights_by_groups['non_toxic'].append(nontoxic)\n",
    "    \n",
    "    def _update_loss_by_groups(self, ce):\n",
    "        toxic, nontoxic = self._something_by_groups(\n",
    "            ce, self.reweighting_metadata)\n",
    "        self.loss_by_groups['toxic'].append(toxic)\n",
    "        self.loss_by_groups['non_toxic'].append(nontoxic)\n",
    "    \n",
    "    def _update_confidence_by_groups(self, p_true):\n",
    "        toxic, nontoxic = self._something_by_groups(\n",
    "            p_true, self.reweighting_metadata, mean=True)\n",
    "        self.confidence_by_groups['toxic'].append(toxic)\n",
    "        self.confidence_by_groups['non_toxic'].append(nontoxic)\n",
    "    \n",
    "    def _update_accs(self, accs, metadata, results, predictions):\n",
    "        toxic, nontoxic = self._something_by_groups(predictions == metadata[:, -2], metadata, mean=True)\n",
    "        accs['mean_acc'].append(results[0]['acc_avg'])\n",
    "        accs['wga'].append(results[0]['acc_wg'])\n",
    "        accs['by_group_toxic'].append(toxic)\n",
    "        accs['by_group_nontoxic'].append(nontoxic)\n",
    "        all_accs = np.array([toxic, nontoxic])\n",
    "        weights = np.array([0.56, 0.29, 0.15])[None, :]\n",
    "        all_accs = np.hstack([\n",
    "            all_accs[:, :3], \n",
    "            np.sum(all_accs[:, 3:6] * weights , axis=1, keepdims=True),\n",
    "            all_accs[:, 6:]])\n",
    "        accs['combined_wga'].append(np.min(all_accs))\n",
    "\n",
    "   \n",
    "    def update(\n",
    "            self, weights, ce, p_true, loss, reg,\n",
    "            reweighting_results, reweighting_predictions,\n",
    "            test_results, test_predictions, \n",
    "            val_results, val_predictions\n",
    "        ):\n",
    "        \n",
    "        self._update_weights_by_groups(weights)\n",
    "        self._update_loss_by_groups(ce)\n",
    "        self._update_confidence_by_groups(p_true)\n",
    "        \n",
    "        self._update_accs(self.reweighting_accs, self.reweighting_metadata, reweighting_results, reweighting_predictions)\n",
    "        self._update_accs(self.test_accs, self.test_metadata, test_results, test_predictions)\n",
    "        self._update_accs(self.val_accs, self.val_metadata, val_results, val_predictions)\n",
    "        \n",
    "        self.losses.append(loss.item())\n",
    "        self.regs.append(reg.item())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wxe(*_):\n",
    "    def confidence_wxe_fn(logits, y, weights):\n",
    "        ce = torch.nn.functional.cross_entropy(logits, y, reduction='none')\n",
    "        loss = weights * ce\n",
    "        return loss.sum(), weights\n",
    "\n",
    "    return confidence_wxe_fn\n",
    "\n",
    "def compute_weights(logits, reweighting_y, gamma, balance_classes, base_weights=0.):\n",
    "    with torch.no_grad():\n",
    "        p = logits.softmax(-1)\n",
    "        y_onehot = torch.zeros_like(logits).scatter_(-1, reweighting_y.unsqueeze(-1), 1)\n",
    "        p_true = (p * y_onehot).sum(-1)\n",
    "        weights = (-gamma * p_true).exp()\n",
    "        if balance_classes:\n",
    "            w1 = (reweighting_y == 0).sum()\n",
    "            w2 = (reweighting_y == 1).sum()\n",
    "            weights[reweighting_y == 0] *= w2 / w1\n",
    "        weights = weights.detach()\n",
    "        weights /= weights.sum()\n",
    "        weights += base_weights / len(weights)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        reweighting_data, test_data, val_data,\n",
    "        emb_dict, num_epochs, gamma, reg_coeff, lr=1e-2,\n",
    "        base_weights=0.,\n",
    "        balance_classes=False, plot=True, silent=False, combine_reweighting=False):\n",
    "\n",
    "    del plot\n",
    "    \n",
    "    reweighting_embeddings = emb_dict['e'].cuda()\n",
    "    reweighting_y = emb_dict['y'].cuda()\n",
    "    reweighting_metadata = emb_dict['m'].cuda()\n",
    "    test_embeddings = emb_dict['test_e'].cuda()\n",
    "    test_y = emb_dict['test_y'].cuda()\n",
    "    test_metadata = emb_dict['test_m'].cuda()\n",
    "    val_embeddings = emb_dict['val_e'].cuda()\n",
    "    val_y = emb_dict['val_y'].cuda()\n",
    "    val_metadata = emb_dict['val_m'].cuda()\n",
    "    w0 = emb_dict['w0'].cuda()\n",
    "    b0 = emb_dict['b0'].cuda()\n",
    "    \n",
    "    if combine_reweighting:\n",
    "        reweighting_embeddings = torch.cat([reweighting_embeddings, val_embeddings])\n",
    "        reweighting_y = torch.cat([reweighting_y, val_y])\n",
    "        reweighting_metadata = torch.cat([reweighting_metadata, val_metadata])\n",
    "    print(\"Reweighting size:\", len(reweighting_y))\n",
    "    \n",
    "    # Early stopping\n",
    "    best_weights = {'w': w0, 'b': b0}\n",
    "    best_wga = 0\n",
    "\n",
    "    last_layer_model = torch.nn.Linear(w0.shape[1], w0.shape[0], bias=True)\n",
    "    last_layer_model.weight.data = w0.clone()\n",
    "    last_layer_model.bias.data = b0.clone()\n",
    "    last_layer_model.cuda()\n",
    "\n",
    "    criterion = get_wxe(gamma)\n",
    "    optimizer = torch.optim.SGD(last_layer_model.parameters(), lr=lr, weight_decay=0., momentum=0.)\n",
    "\n",
    "    metrics = Metrics(reweighting_metadata, test_metadata, val_metadata)\n",
    "\n",
    "    initial_logits = last_layer_model(reweighting_embeddings)\n",
    "    weights = compute_weights(initial_logits, reweighting_y, gamma, balance_classes, base_weights=base_weights)\n",
    "\n",
    "    for epoch in (pbar := tqdm.tqdm(range(num_epochs), disable=silent)):\n",
    "        optimizer.zero_grad()\n",
    "        logits = last_layer_model(reweighting_embeddings)\n",
    "        loss, _ = criterion(logits, reweighting_y, weights)\n",
    "\n",
    "        # L2 penalty for deviation from model.fc.weight and model.fc.bias\n",
    "        reg = ((last_layer_model.weight - w0).pow(2).sum() + (last_layer_model.bias - b0).pow(2).sum())\n",
    "        loss += reg_coeff * reg\n",
    "\n",
    "        # additional infos\n",
    "        with torch.no_grad():\n",
    "            ce = torch.nn.functional.cross_entropy(logits, reweighting_y, reduction='none')\n",
    "            p_true = (logits.softmax(-1) * torch.nn.functional.one_hot(reweighting_y, 2)).sum(-1)\n",
    "            \n",
    "            reweighting_predictions = torch.argmax(logits, -1)\n",
    "            test_predictions = torch.argmax(last_layer_model(test_embeddings), -1)\n",
    "            val_predictions = torch.argmax(last_layer_model(val_embeddings), -1)\n",
    "            reweighting_results = reweighting_data.eval(\n",
    "                    reweighting_predictions.cpu(), reweighting_y.cpu(), reweighting_metadata.cpu())\n",
    "            test_results = test_data.eval(test_predictions.cpu(), test_y.cpu(), test_metadata.cpu())\n",
    "            val_results = val_data.eval(val_predictions.cpu(), val_y.cpu(), val_metadata.cpu())\n",
    "        \n",
    "        metrics.update(\n",
    "            weights, ce, p_true, loss, reg,\n",
    "            reweighting_results, reweighting_predictions,\n",
    "            test_results, test_predictions, \n",
    "            val_results, val_predictions\n",
    "        )\n",
    "        \n",
    "        wga = metrics.val_accs['combined_wga'][-1]\n",
    "        if wga > best_wga:\n",
    "            best_wga = wga\n",
    "            best_weights = {'w': last_layer_model.weight.data.clone(), 'b': last_layer_model.bias.data.clone()}\n",
    "\n",
    "        nzw = (weights != 0).float().mean()\n",
    "        loss.backward()\n",
    "        # clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(last_layer_model.parameters(), 1.)\n",
    "        optimizer.step()\n",
    "    #     scheduler.step()\n",
    "\n",
    "        wga = metrics.reweighting_accs[\"wga\"][-1]\n",
    "        test_wga = metrics.test_accs[\"wga\"][-1]\n",
    "        pbar.set_description(f\"Val cwga: {best_wga:.3f}, TWGA: {test_wga:.3f}\")\n",
    "\n",
    "    return metrics, best_weights, best_wga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_size_adjusted_wga(accs, sizes, i):\n",
    "    at, st = np.array(accs[\"by_group_toxic\"][i]), np.array(sizes[\"toxic\"])\n",
    "    an, sn = np.array(accs[\"by_group_nontoxic\"][i]), np.array(sizes[\"non_toxic\"])\n",
    "    vt, vn = at - at**2, an - an**2\n",
    "    lbs = np.concatenate([at - np.sqrt(vt / st), an - np.sqrt(vn / sn)])\n",
    "    return np.min(lbs)\n",
    "\n",
    "def wga(accs, i):\n",
    "    at = np.array(accs[\"by_group_toxic\"][i])\n",
    "    an = np.array(accs[\"by_group_nontoxic\"][i])\n",
    "    return np.min(np.concatenate([at, an]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twga(metrics):\n",
    "    earlystop_epoch = np.argmax(metrics.val_accs[\"combined_wga\"])\n",
    "    return metrics.test_accs['wga'][earlystop_epoch]\n",
    "\n",
    "def twga_last(metrics):\n",
    "    earlystop_epoch = -1\n",
    "    return metrics.test_accs['wga'][earlystop_epoch]\n",
    "\n",
    "def vwga(metrics):\n",
    "    earlystop_epoch = np.argmax(metrics.val_accs[\"wga\"])\n",
    "    return metrics.val_accs['wga'][earlystop_epoch]\n",
    "\n",
    "def vwga_last(metrics):\n",
    "    earlystop_epoch = -1\n",
    "    return metrics.val_accs['wga'][earlystop_epoch]\n",
    "\n",
    "def vcwga(metrics):\n",
    "    earlystop_epoch = np.argmax(metrics.val_accs[\"combined_wga\"])\n",
    "    return metrics.val_accs['combined_wga'][earlystop_epoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [0., 0.001, 0.01, 0.1, 1., 10., 100.]\n",
    "reg_coefs = [0]\n",
    "lrs = [1e-2]\n",
    "vps = [1.] # validation proportion: change / add more if you want to subsample the validation set\n",
    "\n",
    "all_results = {}\n",
    "all_metrics = {}\n",
    "\n",
    "for val_prop in vps:\n",
    "    for seed in seeds:\n",
    "        for shuffle_seed in [0, 21, 42]:\n",
    "            rng = np.random.default_rng(shuffle_seed)\n",
    "            emb_dict = torch.load(f\"{save_dir}/{seed}.pt\")\n",
    "            idx = np.arange(len(emb_dict['val_y']))\n",
    "            rng.shuffle(idx)\n",
    "            idx = idx[:int(val_prop * len(idx))]\n",
    "            emb_dict['val_y'] = emb_dict['val_y'][idx]\n",
    "            emb_dict['val_e'] = emb_dict['val_e'][idx]\n",
    "            emb_dict['val_m'] = emb_dict['val_m'][idx]\n",
    "            for gamma in gammas:\n",
    "                for reg_coef in reg_coefs:\n",
    "                    for lr in lrs:\n",
    "                        metrics, weights, val_wga = train(\n",
    "                                reweighting_data, test_data, val_data,\n",
    "                                emb_dict, num_epochs=50, gamma=gamma, reg_coeff=reg_coef, lr=lr,\n",
    "                                balance_classes=True, plot=False, silent=False, combine_reweighting=False)\n",
    "                        all_results[(val_prop, shuffle_seed, seed, gamma, reg_coef, lr)] = val_wga\n",
    "                        all_metrics[(val_prop, shuffle_seed, seed, gamma, reg_coef, lr)] = metrics\n",
    "                        print(f\"seed {seed}, gamma {gamma}, reg {reg_coef}, lr {lr}: {val_wga}\")\n",
    "# save all_metrics\n",
    "torch.save(all_metrics, \"all_metrics.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select best hyperparameters\n",
    "all_metrics = torch.load(\"all_metrics.pt\")\n",
    "seeds = [0,1,2]\n",
    "test_wgas = []\n",
    "for val_prop in vps:\n",
    "    twgas = []\n",
    "    for seed in seeds:\n",
    "        seed_metrics = {k: v for (k, v) in all_metrics.items() if k[0] == val_prop and k[2] == seed}\n",
    "        max_key = max(seed_metrics, key=lambda k: vcwga(seed_metrics[k]))\n",
    "        twgas.append(twga(all_metrics[max_key]))\n",
    "    test_wgas.append(twgas)\n",
    "    print(val_prop, np.mean(twgas), np.std(twgas))\n",
    "\n",
    "mean_wgas = [np.mean(twgas) for twgas in test_wgas]\n",
    "std_wgas = [np.std(twgas) for twgas in test_wgas]\n",
    "mean_wgas = np.array(mean_wgas) * 100\n",
    "std_wgas = np.array(std_wgas) * 100\n",
    "vps = np.array(vps)\n",
    "\n",
    "print(mean_wgas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
