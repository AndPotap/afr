{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env OMP_NUM_THREADS=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Modify these to your own paths ###\n",
    "wb_data_path = '/datasets/waterbirds_official'\n",
    "celeba_data_path = '/datasets/CelebA'\n",
    "\n",
    "CKPTDIR = '/home/shikai_q/spurious_old/checkpoints'\n",
    "\n",
    "WB_CKPTS = [\n",
    "    f\"{CKPTDIR}/wb/80_1/final_checkpoint.pt\", \n",
    "    f\"{CKPTDIR}/wb/80_21/20230115_131136_mVno/final_checkpoint.pt\",\n",
    "    f\"{CKPTDIR}/wb/80_98/20230115_131150_CThg/final_checkpoint.pt\"\n",
    "]\n",
    "CelebA_CKPTS = [\n",
    "    f\"{CKPTDIR}/celeba/80_1/20230106_144116_tpaT/final_checkpoint.pt\", \n",
    "    f\"{CKPTDIR}/celeba/80_21/20230106_144132_glNH/final_checkpoint.pt\",\n",
    "    f\"{CKPTDIR}/celeba/80_98/20230106_144147_AVRm/final_checkpoint.pt\"\n",
    "    ]\n",
    "### The remaining code needs zero modification ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "\n",
    "# import from parent directory\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "import models\n",
    "import utils\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(ckpt, args, num_augs, load_emb=True, model_cls=models.imagenet_resnet50_pretrained):\n",
    "    trian_group_ratios = np.array(utils.get_train_group_ratios(args))\n",
    "    train_loader, holdout_loaders = utils.get_data(args, finetune_on_val=args.finetune_on_val)\n",
    "    n_classes = train_loader.dataset.n_classes\n",
    "\n",
    "    model = model_cls(n_classes)\n",
    "    model.load_state_dict(torch.load(ckpt))\n",
    "    model.cuda(); model.eval();\n",
    "\n",
    "    classifier = model.fc\n",
    "    w0 = classifier.weight.data.clone()\n",
    "    b0 = classifier.bias.data.clone()\n",
    "    model.fc = torch.nn.Identity()\n",
    "    \n",
    "    emb_path = f'{ckpt.replace(\".pt\", f\"_{args.data_transform}{num_augs}_embeddings.pt\")}'\n",
    "    if not load_emb or not os.path.exists(emb_path):\n",
    "        reweighting_embeddings, reweighting_predictions, reweighting_y, reweighting_groups = get_embeddings_predictions(model, classifier, train_loader, num_augs)\n",
    "        val_embeddings, val_predictions, val_y, val_groups = get_embeddings_predictions(model, classifier, holdout_loaders['val'], num_augs=1)\n",
    "        test_embeddings, test_predictions, test_y, test_groups = get_embeddings_predictions(model, classifier, holdout_loaders['test'], num_augs=1)\n",
    "        emb_dict = dict(\n",
    "                e=reweighting_embeddings, y=reweighting_y, pred=reweighting_predictions, g=reweighting_groups,\n",
    "                test_e=test_embeddings, test_pred=test_predictions, test_y=test_y, test_g=test_groups,\n",
    "                val_e=val_embeddings, val_pred=val_predictions, val_y=val_y, val_g=val_groups,\n",
    "                w0=w0, b0=b0, trian_group_ratios=trian_group_ratios\n",
    "            )\n",
    "        torch.save(\n",
    "            emb_dict,\n",
    "            emb_path\n",
    "        )\n",
    "        print(f'Saved embeddings to {emb_path}')\n",
    "    else:\n",
    "        print(f'Loading embeddings from {emb_path}')\n",
    "        emb_dict = torch.load(emb_path)\n",
    "        reweighting_embeddings, reweighting_predictions, reweighting_y, reweighting_groups = emb_dict['e'], emb_dict['pred'], emb_dict['y'], emb_dict['g']\n",
    "        test_embeddings, test_predictions, test_y, test_groups = emb_dict['test_e'], emb_dict['test_pred'], emb_dict['test_y'], emb_dict['test_g']\n",
    "        val_embeddings, val_predictions, val_y, val_groups = emb_dict['val_e'], emb_dict['val_pred'], emb_dict['val_y'], emb_dict['val_g']\n",
    "\n",
    "    group_counts = {}\n",
    "    for g in reweighting_groups:\n",
    "            group_counts[g.item()] = group_counts.get(g.item(), 0) + 1\n",
    "    print('*** Reweighting Counts + Group distribution: ***')\n",
    "    for g in sorted(group_counts.keys()):\n",
    "        print(f'Group {g}: {group_counts[g]} ({(group_counts[g] / sum(group_counts.values()) * 100):.1f}%)')\n",
    "\n",
    "    print('*** Reweighting Acc + Group distribution: ***')        \n",
    "    print_accs(reweighting_predictions, reweighting_y, reweighting_groups)\n",
    "\n",
    "    print('*** Test Acc + Group distribution: ***')        \n",
    "    print_accs(test_predictions, test_y, test_groups)\n",
    "\n",
    "    print('*** Val Acc + Group distribution: ***')\n",
    "    print_accs(val_predictions, val_y, val_groups)\n",
    "    # return reweighting_embeddings.cuda(), reweighting_y.cuda(), reweighting_groups.cuda(), test_embeddings.cuda(), test_y.cuda(), test_predictions.cuda(), test_groups.cuda(), \n",
    "    emb_dict['trian_group_ratios'] = trian_group_ratios\n",
    "    return emb_dict\n",
    "\n",
    "def get_embeddings_predictions(feature_extractor, classifier, loader, num_augs):\n",
    "    all_embeddings, all_predictions, all_y_true, all_groups = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for epoch in range(num_augs):\n",
    "            for x, y_true, g, *_ in tqdm.tqdm(loader):\n",
    "                x = x.cuda()\n",
    "\n",
    "                embeddings = feature_extractor(x)\n",
    "                predictions = torch.argmax(classifier(embeddings), axis=1)\n",
    "                all_embeddings.append(embeddings.cpu())\n",
    "                all_predictions.append(predictions.cpu())\n",
    "                all_y_true.append(y_true.cpu())\n",
    "                all_groups.append(g.cpu())\n",
    "    all_embeddings = torch.cat(all_embeddings, axis=0)\n",
    "    all_predictions = torch.cat(all_predictions, axis=0)\n",
    "    all_y_true = torch.cat(all_y_true, axis=0)\n",
    "    all_groups = torch.cat(all_groups, axis=0)\n",
    "    return all_embeddings, all_predictions, all_y_true, all_groups\n",
    "\n",
    "def get_accs(test_predictions, test_y, test_groups):\n",
    "    return [(test_predictions == test_y)[test_groups == g].float().mean().item() for g in range(test_groups.max() + 1)]\n",
    "\n",
    "def print_accs(test_predictions, test_y, test_groups):\n",
    "    acc = (test_predictions == test_y).float().mean()\n",
    "    group_accs = get_accs(test_predictions, test_y, test_groups)\n",
    "    group_counts = {}\n",
    "    for g in test_groups:\n",
    "            group_counts[g.item()] = group_counts.get(g.item(), 0) + 1\n",
    "    print(f\"Avg: {acc:.3f}\")\n",
    "    for g, acc in enumerate(group_accs):\n",
    "        if g in group_counts:\n",
    "            print(f\"Group {g}: {acc:.2f} ({(group_counts[g] / sum(group_counts.values()) * 100):.1f}%)\")\n",
    "        else:\n",
    "            print(f\"Group {g}: {acc:.2f} (0.0%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wxe_fn(logits, y, weights):\n",
    "    ce = torch.nn.functional.cross_entropy(logits, y, reduction='none')\n",
    "    l = weights * ce\n",
    "    return l.sum()\n",
    "\n",
    "def compute_afr_weights(erm_logits, class_label, gamma, balance_classes):\n",
    "    # erm_logits: (n_samples, n_classes)\n",
    "    # class_label: (n_samples,)\n",
    "    # gamma: float\n",
    "    with torch.no_grad():\n",
    "        p = erm_logits.softmax(-1)\n",
    "        y_onehot = torch.zeros_like(erm_logits).scatter_(-1, class_label.unsqueeze(-1), 1)\n",
    "        p_true = (p * y_onehot).sum(-1)\n",
    "        weights = (-gamma * p_true).exp()\n",
    "        n_classes = torch.unique(class_label).numel()\n",
    "        # class balancing\n",
    "        if balance_classes:\n",
    "            class_count = []\n",
    "            for y in range(n_classes):\n",
    "                class_count.append((class_label == y).sum())\n",
    "            for y in range(1, n_classes):\n",
    "                weights[class_label == y] *= class_count[0] / class_count[y]\n",
    "        weights /= weights.sum()\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(emb_dict, num_epochs, gamma, reg_coeff, lr=1e-2, balance_classes=False, early_stop='wga', plot=True, silent=False, group_uniform=False, opt='sgd'):\n",
    "    reweighting_embeddings = emb_dict['e'].cuda()\n",
    "    reweighting_y = emb_dict['y'].cuda()\n",
    "    reweighting_groups = emb_dict['g'].cuda()\n",
    "    test_embeddings = emb_dict['test_e'].cuda()\n",
    "    test_y = emb_dict['test_y'].cuda()\n",
    "    test_groups = emb_dict['test_g'].cuda()\n",
    "    val_embeddings = emb_dict['val_e'].cuda()\n",
    "    val_y = emb_dict['val_y'].cuda()\n",
    "    val_groups = emb_dict['val_g'].cuda()\n",
    "    w0 = emb_dict['w0'].cuda()\n",
    "    b0 = emb_dict['b0'].cuda()\n",
    "    trian_group_ratios = emb_dict['trian_group_ratios']\n",
    "    num_groups = torch.unique(test_groups).numel()\n",
    "\n",
    "    class Model(torch.nn.Module):\n",
    "        def __init__(self, w0, b0):\n",
    "            super().__init__()\n",
    "            self.w0 = w0\n",
    "            self.b0 = b0\n",
    "            self.linear = torch.nn.Linear(w0.shape[1], w0.shape[0], bias=True)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            y_old = x @ self.w0.t() + self.b0\n",
    "            y_new = self.linear(x)\n",
    "            return y_old + y_new\n",
    "\n",
    "    model = Model(w0, b0)\n",
    "    model.cuda()\n",
    "\n",
    "    criterion = wxe_fn\n",
    "    if opt == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0., momentum=0.)\n",
    "    elif opt == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    mean_accs = []\n",
    "    test_mean_accs = []\n",
    "    mean_group_accs = []\n",
    "    val_mean_group_accs = []\n",
    "    wgas = []\n",
    "    test_wgas = []\n",
    "    val_wgas = []\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    regs = []\n",
    "    weights_by_groups = []\n",
    "    acc_by_groups = []\n",
    "    val_acc_by_groups = []\n",
    "    test_accs_by_groups = []\n",
    "    weighted_acc = []\n",
    "    val_weighted_acc = []\n",
    "\n",
    "    initial_logits = reweighting_embeddings @ w0.t() + b0\n",
    "    weights = compute_afr_weights(initial_logits, reweighting_y, gamma, balance_classes)\n",
    "    if group_uniform:\n",
    "        # uniform total weights per group\n",
    "        group_counts = torch.bincount(reweighting_groups)\n",
    "        for g in range(len(group_counts)):\n",
    "            weights[reweighting_groups == g] = 1 / group_counts[g] / len(group_counts)\n",
    "    for _ in (pbar := tqdm.tqdm(range(num_epochs), disable=silent)):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(reweighting_embeddings)\n",
    "        loss = criterion(logits, reweighting_y, weights)\n",
    "        reg = model.linear.weight.pow(2).sum() + model.linear.bias.pow(2).sum()\n",
    "        loss += reg_coeff * reg\n",
    "\n",
    "        loss.backward()\n",
    "        # clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "        optimizer.step()\n",
    "        # additional infos\n",
    "        with torch.no_grad():\n",
    "            val_logits = model(val_embeddings)\n",
    "            val_weights = compute_afr_weights(val_logits, val_y, gamma, balance_classes)\n",
    "            ce = torch.nn.functional.cross_entropy(logits, reweighting_y, reduction='none')\n",
    "            accs = get_accs(torch.argmax(logits, -1), reweighting_y, reweighting_groups)\n",
    "            val_accs = get_accs(torch.argmax(model(val_embeddings), -1), val_y, val_groups)\n",
    "            test_accs = get_accs(torch.argmax(model(test_embeddings), -1), test_y, test_groups)\n",
    "        \n",
    "            weights_by_group = [weights[reweighting_groups == g].sum().item() for g in range(num_groups)]\n",
    "            weights_by_groups.append(weights_by_group)\n",
    "            acc_by_groups.append(accs)\n",
    "            val_acc_by_groups.append(val_accs)\n",
    "            test_accs_by_groups.append(test_accs)\n",
    "            mean_acc = (np.array(accs) * np.array(trian_group_ratios)).sum()\n",
    "            mean_group_accs.append(np.array(accs).mean())\n",
    "            mean_accs.append(mean_acc)\n",
    "            val_mean_group_accs.append(np.array(val_accs).mean())\n",
    "            wga = min(accs)\n",
    "            test_wga = min(test_accs)\n",
    "            val_wga = min(val_accs)\n",
    "            wgas.append(wga)\n",
    "            val_wgas.append(val_wga)\n",
    "            test_wgas.append(test_wga)\n",
    "            # test mean acc is test_accs weighted by trian_group_ratios\n",
    "            test_mean_acc = (np.array(test_accs) * np.array(trian_group_ratios)).sum()\n",
    "            test_mean_accs.append(test_mean_acc)\n",
    "            losses.append(loss.item())\n",
    "            regs.append(reg.item())\n",
    "            weighted_acc.append((((torch.argmax(logits, -1) == reweighting_y).float() * weights).sum() / weights.sum()).item())\n",
    "            val_weighted_acc.append((((torch.argmax(val_logits, -1) == val_y).float() * val_weights).sum() / val_weights.sum()).item())\n",
    "            val_losses.append(criterion(val_logits, val_y, val_weights).item())\n",
    "            pbar.set_description(f\"Loss: {loss.item():.5f}, WGA: {wga:.3f}, TWGA: {test_wga:.3f}\")\n",
    "\n",
    "    \n",
    "    if early_stop == 'wga':\n",
    "        earlystop_epoch = np.argmax(val_wgas)\n",
    "        early_stop_metric = np.max(val_wgas)\n",
    "    elif early_stop == 'wga@max_val_wa':\n",
    "        earlystop_epoch = np.argmax(val_weighted_acc)\n",
    "        early_stop_metric = val_wgas[earlystop_epoch]\n",
    "    elif early_stop == 'mga':\n",
    "        earlystop_epoch = np.argmax(val_mean_group_accs)\n",
    "        early_stop_metric = np.max(val_mean_group_accs)\n",
    "    elif early_stop == 'none':\n",
    "        earlystop_epoch = num_epochs - 1\n",
    "        early_stop_metric = val_wgas[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown early_stop: {early_stop}\")\n",
    "    earlystop_twga = test_wgas[earlystop_epoch]\n",
    "    earlystop_tmacc = test_mean_accs[earlystop_epoch]\n",
    "    final_twga = test_wgas[-1]\n",
    "    final_tmacc = test_mean_accs[-1]\n",
    "    if plot:\n",
    "        # print test wga at best reweighting wga epoch\n",
    "        print(f\"Final test WGA: {final_twga:.3f}\")\n",
    "        print(f\"Final test mean acc: {final_tmacc:.3f}\")\n",
    "        print(f\"Early stopping test WGA: {earlystop_twga:.3f} at epoch {earlystop_epoch}\") \n",
    "        print(f\"Early stopping test mean acc: {earlystop_tmacc:.3f} at epoch {earlystop_epoch}\")\n",
    "\n",
    "        # 3 horizontal subplots\n",
    "        fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20, 5))\n",
    "        ax1.plot(losses, label=\"CWXE\")\n",
    "        ax1.plot(regs, label=\"Reg\")\n",
    "        ax1.grid()\n",
    "        ax1.legend(loc=\"lower right\")\n",
    "        ax1.set_xlabel(\"Epochs\")\n",
    "        ax1.set_ylabel(\"Loss\")\n",
    "\n",
    "        ax2.plot(test_wgas, label=\"Test\")\n",
    "        ax2.plot(wgas, label=\"Reweighting\")\n",
    "        ax2.plot(val_wgas, label=\"Validation\")\n",
    "        ax2.grid()\n",
    "        ax2.legend(loc=\"lower right\")\n",
    "        ax2.set_xlabel(\"Epochs\")\n",
    "        ax2.set_ylabel(\"WGA\")\n",
    "\n",
    "        ax3.plot(test_mean_accs, label=\"Test\")\n",
    "        ax3.plot(mean_accs, label=\"Reweighting\")\n",
    "        ax3.grid()\n",
    "        ax3.legend(loc=\"lower right\")\n",
    "        ax3.set_xlabel(\"Epochs\")\n",
    "        ax3.set_ylabel(\"Mean Acc\")\n",
    "\n",
    "        ax4.plot(weighted_acc, label=\"Reweighting\")\n",
    "        ax4.plot(val_weighted_acc, label=\"Validation\")\n",
    "        ax4.grid()\n",
    "        ax4.set_xlabel(\"Epochs\")\n",
    "        ax4.set_ylabel(\"Weighted Acc\")\n",
    "        ax4.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "        # 3 horizontal subplots\n",
    "        fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "        ax1.plot(weights_by_groups, label=[f'G{g+1}' for g in range(num_groups)])\n",
    "        ax1.grid()\n",
    "        ax1.set_xlabel(\"Epochs\")\n",
    "        ax1.set_ylabel(\"Weights\")\n",
    "        ax1.legend(loc=\"lower right\")\n",
    "\n",
    "        for g in range(num_groups):\n",
    "            ax2.plot([acc[g] for acc in acc_by_groups], label=f\"G{g+1}\", color=f\"C{g}\")\n",
    "            ax2.plot([acc[g] for acc in test_accs_by_groups], color=f\"C{g}\", linestyle=\":\")\n",
    "        ax2.grid()\n",
    "        ax2.set_xlabel(\"Epochs\")\n",
    "        ax2.set_ylabel(\"Acc\")\n",
    "        ax2.legend(loc=\"lower right\")\n",
    "\n",
    "    return early_stop_metric, earlystop_twga, earlystop_tmacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_sampler(sweep_config, random=True):\n",
    "    if random:\n",
    "        while True:\n",
    "            lr = np.random.choice(sweep_config[\"lr_choices\"])\n",
    "            gamma = np.random.choice(sweep_config[\"gamma_choices\"])\n",
    "            reg_coeff = np.random.choice(sweep_config[\"reg_coeff_choices\"])\n",
    "            balance_classes = np.random.choice(sweep_config[\"balance_classes_choices\"])\n",
    "            yield lr, gamma, reg_coeff, balance_classes\n",
    "    else:\n",
    "        for lr in sweep_config[\"lr_choices\"]:\n",
    "            for gamma in sweep_config[\"gamma_choices\"]:\n",
    "                for reg_coeff in sweep_config[\"reg_coeff_choices\"]:\n",
    "                    for balance_classes in sweep_config[\"balance_classes_choices\"]:\n",
    "                        yield lr, gamma, reg_coeff, balance_classes\n",
    "\n",
    "def run_training(emb_dict, num_epochs, lr, gamma, reg_coeff, balance_classes, early_stop):\n",
    "    return train(emb_dict, num_epochs=num_epochs, gamma=gamma, reg_coeff=reg_coeff, lr=lr, balance_classes=balance_classes, early_stop=early_stop, plot=False, silent=True), (lr, gamma, reg_coeff, balance_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep(ckpts, args, num_augs, sweep_config, load_emb, skip_if_done, seed=0):\n",
    "    best_twgas = []\n",
    "    best_tmaccs = []\n",
    "    for ckpt in ckpts:\n",
    "        emb_dict = extract_embeddings(ckpt, args, num_augs, load_emb)\n",
    "        # same dir but with {sweep_name}.pt\n",
    "        sweep_file = os.path.join(os.path.dirname(ckpt), sweep_config['sweep_name'] + '.pt')\n",
    "\n",
    "        if skip_if_done and os.path.exists(sweep_file):\n",
    "            twga = torch.load(sweep_file)['best_twga']\n",
    "            print(f\"Skipping {ckpt}: TWGA {twga:.3f}\")\n",
    "            best_twgas.append(twga)\n",
    "            best_tmaccs.append(torch.load(sweep_file)['best_tmacc'])\n",
    "            continue\n",
    "\n",
    "        # break\n",
    "        # find best hyperparameters\n",
    "        best_vwga = 0\n",
    "        best_twga = 0\n",
    "        best_tmacc = 0\n",
    "        best_hyperparams = None\n",
    "        top10_choices = []\n",
    "        all_results = []\n",
    "        \n",
    "        # fix all seeds\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        import random\n",
    "        random.seed(seed)\n",
    "        \n",
    "        # subsample validation set\n",
    "        if sweep_config['val_prop'] < 1:\n",
    "            val_size = int(emb_dict['val_e'].shape[0] * sweep_config['val_prop'])\n",
    "            val_idx = np.random.choice(emb_dict['val_e'].shape[0], val_size, replace=False)\n",
    "            emb_dict['val_e'] = emb_dict['val_e'][val_idx]\n",
    "            emb_dict['val_y'] = emb_dict['val_y'][val_idx]\n",
    "            emb_dict['val_g'] = emb_dict['val_g'][val_idx]\n",
    "        print(f'Tuning hypers on {emb_dict[\"val_e\"].shape[0]} val examples')\n",
    "        sampler = hyper_sampler(sweep_config, random=sweep_config['random'])\n",
    "        for i in (pbar:= tqdm.tqdm(range(sweep_config['n_trials']))):\n",
    "            try:\n",
    "                lr, gamma, reg_coeff, balance_classes = next(sampler)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            (val_wga, test_wga, test_macc), hyperparams = run_training(emb_dict, sweep_config['num_epochs'], lr, gamma, reg_coeff, balance_classes, sweep_config['early_stop'])\n",
    "            all_results.append([gamma, reg_coeff, val_wga, test_wga])\n",
    "            if val_wga > best_vwga:\n",
    "                best_vwga = val_wga\n",
    "                best_twga = test_wga\n",
    "                best_tmacc = test_macc\n",
    "                best_hyperparams = hyperparams\n",
    "                lr, gamma, reg_coeff, balance_classes = best_hyperparams\n",
    "                pbar.set_description(f\"Test WGA: {test_wga:.3f}, Val WGA: {val_wga:.3f}, lr: {lr:.2g}, gamma: {gamma:.3f}, reg_coeff: {reg_coeff:.3f}, balance_classes: {balance_classes}\")\n",
    "            if len(top10_choices) < 10:\n",
    "                top10_choices.append((test_wga, val_wga, hyperparams))\n",
    "                top10_choices = sorted(top10_choices, key=lambda x: x[1], reverse=True)\n",
    "            else:\n",
    "                top10_choices = sorted(top10_choices, key=lambda x: x[1], reverse=True)\n",
    "                if val_wga > top10_choices[-1][1]:\n",
    "                    top10_choices[-1] = (test_wga, val_wga, hyperparams)\n",
    "\n",
    "        # save sweep results, indicate data_transform and num aug\n",
    "        torch.save({\n",
    "            'best_twga': best_twga,\n",
    "            'best_tmacc': best_tmacc,\n",
    "            'best_hyperparams': best_hyperparams,\n",
    "            'top10_choices': top10_choices,\n",
    "            'sweep_config': sweep_config,\n",
    "            'all_results': all_results,\n",
    "        }, sweep_file)\n",
    "        best_twgas.append(best_twga)\n",
    "        best_tmaccs.append(best_tmacc)\n",
    "\n",
    "    # print mean and std of best WGA\n",
    "    print(f\"Mean best WGA: {np.mean(best_twgas):.3f}, std: {np.std(best_twgas):.3f}\")\n",
    "    # print mean and std of mean acc\n",
    "    print(f\"Mean best mean acc: {np.mean(best_tmaccs):.3f}, std: {np.std(best_tmaccs):.3f}\")\n",
    "    return np.mean(best_twgas), np.std(best_twgas), np.mean(best_tmaccs), np.std(best_tmaccs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## WB ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_emb = True\n",
    "skip_if_done = True\n",
    "num_augs = 1\n",
    "val_prop = 1\n",
    "\n",
    "args = SimpleNamespace(\n",
    "        data_transform='NoAugWaterbirdsCelebATransform',\n",
    "        dataset='SpuriousDataset',\n",
    "        data_dir=wb_data_path,\n",
    "        num_minority_groups_remove=0,\n",
    "        test_data_dir=None,\n",
    "        val_size=-1,\n",
    "        mixup=False,\n",
    "        batch_size=32,\n",
    "        reweight_groups=False,\n",
    "        reweight_classes=False,\n",
    "        reweight_spurious=False,\n",
    "        no_shuffle_train=False,\n",
    "        finetune_on_val=False,\n",
    "        dfr=False,\n",
    "        pass_n=0,\n",
    "        max_prop=1,\n",
    "        train_prop=-0.2,\n",
    "        val_prop=1, # subsampling happens in sweep function\n",
    "    )\n",
    "sweep_config = {\n",
    "    \"n_trials\": 33 * 4,\n",
    "    \"num_epochs\": 500,\n",
    "    \"lr_choices\": [1e-2],\n",
    "    \"gamma_choices\": np.linspace(4, 20, 33),\n",
    "    \"reg_coeff_choices\": [0, 0.1, 0.2, 0.3],\n",
    "    \"balance_classes_choices\": [True],\n",
    "    \"early_stop\": 'wga',\n",
    "    \"random\": False,\n",
    "    \"val_prop\": val_prop,\n",
    "    \"sweep_name\": 'nominal'\n",
    "    }\n",
    "sweep(WB_CKPTS, args, num_augs, sweep_config, load_emb, skip_if_done)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## CelebA ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_emb = True\n",
    "skip_if_done = True\n",
    "num_augs = 1\n",
    "val_prop = 1\n",
    "\n",
    "args = SimpleNamespace(\n",
    "        data_transform='NoAugWaterbirdsCelebATransform',\n",
    "        dataset='SpuriousDataset',\n",
    "        data_dir=celeba_data_path,\n",
    "        num_minority_groups_remove=0,\n",
    "        test_data_dir=None,\n",
    "        val_size=-1,\n",
    "        mixup=False,\n",
    "        batch_size=32,\n",
    "        reweight_groups=False,\n",
    "        reweight_classes=False,\n",
    "        reweight_spurious=False,\n",
    "        no_shuffle_train=False,\n",
    "        finetune_on_val=False,\n",
    "        dfr=False,\n",
    "        pass_n=0,\n",
    "        max_prop=1,\n",
    "        train_prop=-0.2,\n",
    "        val_prop=1, # subsampling happens in sweep function\n",
    "    )\n",
    "sweep_config = {\n",
    "    \"n_trials\": 30,\n",
    "    \"num_epochs\": 1000,\n",
    "    \"lr_choices\": [2e-2],\n",
    "    \"gamma_choices\": np.linspace(1, 3, 10),\n",
    "    \"reg_coeff_choices\": np.logspace(-3, -1, 3),\n",
    "    \"balance_classes_choices\": [True],\n",
    "    \"early_stop\": 'wga',\n",
    "    \"random\": False, # if True then random search, else grid search\n",
    "    \"val_prop\": val_prop,\n",
    "    \"sweep_name\": 'nominal'\n",
    "    }\n",
    "sweep(CelebA_CKPTS, args, num_augs, sweep_config, load_emb, skip_if_done)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robustness to $\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WB\n",
    "args = SimpleNamespace(\n",
    "        data_transform='NoAugWaterbirdsCelebATransform',\n",
    "        dataset='SpuriousDataset',\n",
    "        data_dir=wb_data_path,\n",
    "        num_minority_groups_remove=0,\n",
    "        test_data_dir=None,\n",
    "        val_size=-1,\n",
    "        mixup=False,\n",
    "        batch_size=32,\n",
    "        reweight_groups=False,\n",
    "        reweight_classes=False,\n",
    "        reweight_spurious=False,\n",
    "        no_shuffle_train=False,\n",
    "        finetune_on_val=False,\n",
    "        dfr=False,\n",
    "        pass_n=0,\n",
    "        max_prop=1,\n",
    "        train_prop=-0.2,\n",
    "        val_prop=1\n",
    "    )\n",
    "\n",
    "num_augs = 1\n",
    "\n",
    "gammas = np.linspace(0, 30, 11)\n",
    "\n",
    "twgas = []\n",
    "n_effs = []\n",
    "for gamma in tqdm.tqdm(gammas):\n",
    "    twga = []\n",
    "    n_eff = []\n",
    "    accs_per_gs = []\n",
    "    for ckpt in WB_CKPTS:\n",
    "        emb_dict = extract_embeddings(ckpt, args, num_augs, load_emb=True)\n",
    "        _, this_twga, _ = train(emb_dict, num_epochs=500, gamma=gamma, reg_coeff=0, lr=1e-2, balance_classes=True, early_stop='wga', plot=False, silent=True)\n",
    "        e = emb_dict['e'].cuda()\n",
    "        y = emb_dict['y'].cuda()\n",
    "        g = emb_dict['g'].cuda()\n",
    "        w0 = emb_dict['w0'].cuda()\n",
    "        b0 = emb_dict['b0'].cuda()\n",
    "        logits = e @ w0.t() + b0\n",
    "        weights = compute_afr_weights(logits, y, gamma, True)\n",
    "        n_eff.append(1 / (weights ** 2).sum().item())\n",
    "        twga.append(this_twga)\n",
    "    twgas.append(twga)\n",
    "    n_effs.append(n_eff)\n",
    "twgas = np.array(twgas)\n",
    "n_effs = np.array(n_effs)\n",
    "\n",
    "mean_wgas = twgas.mean(axis=1) * 100\n",
    "std_wgas = twgas.std(axis=1) * 100\n",
    "mean_n_effs = n_effs.mean(axis=1)\n",
    "std_n_effs = n_effs.std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", font_scale=2.0, rc={\"lines.linewidth\": 3.0})\n",
    "sns.set_palette(\"Set1\")\n",
    "\n",
    "plt.figure(dpi=100, figsize=(8, 6))\n",
    "plt.fill_between(gammas, mean_wgas - std_wgas, mean_wgas + std_wgas, alpha=0.2)\n",
    "plt.plot(gammas, mean_wgas, label='AFR', marker='o', markersize=10)\n",
    "plt.xlabel(r'$\\gamma$')\n",
    "\n",
    "erm = 72.6\n",
    "\n",
    "plt.plot([0, max(gammas)], [erm, erm], '--', label='ERM')\n",
    "plt.ylim(bottom=60)\n",
    "\n",
    "plt.ylabel('Test WGA [%]')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../plots/wb_wga_vs_gamma.pdf')\n",
    "\n",
    "\n",
    "plt.figure(dpi=100, figsize=(8, 6))\n",
    "plt.fill_between(gammas, mean_n_effs - std_n_effs, mean_n_effs + std_n_effs, alpha=0.2)\n",
    "plt.plot(gammas, mean_n_effs, marker='o', markersize=10)\n",
    "plt.xlabel(r'$\\gamma$')\n",
    "plt.ylabel(r'Effective # Samples')\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('../plots/wb_effsize_vs_gamma.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CelebA\n",
    "args = SimpleNamespace(\n",
    "        data_transform='NoAugWaterbirdsCelebATransform',\n",
    "        dataset='SpuriousDataset',\n",
    "        data_dir = celeba_data_path,\n",
    "        num_minority_groups_remove=0,\n",
    "        test_data_dir=None,\n",
    "        val_size=-1,\n",
    "        mixup=False,\n",
    "        batch_size=32,\n",
    "        reweight_groups=False,\n",
    "        reweight_classes=False,\n",
    "        reweight_spurious=False,\n",
    "        no_shuffle_train=False,\n",
    "        finetune_on_val=False,\n",
    "        dfr=False,\n",
    "        pass_n=0,\n",
    "        max_prop=1,\n",
    "        train_prop=-0.2,\n",
    "        val_prop=1,\n",
    "    )\n",
    "num_augs = 1\n",
    "\n",
    "gammas = np.linspace(0, 30, 11)\n",
    "\n",
    "twgas = []\n",
    "n_effs = []\n",
    "for gamma in tqdm.tqdm(gammas):\n",
    "    twga = []\n",
    "    n_eff = []\n",
    "    accs_per_gs = []\n",
    "    for ckpt in CelebA_CKPTS:\n",
    "        emb_dict = extract_embeddings(ckpt, args, num_augs, load_emb=True)\n",
    "        _, this_twga, _ = train(emb_dict, num_epochs=500, gamma=gamma, reg_coeff=0, lr=1e-2, balance_classes=True, early_stop='wga', plot=False, silent=True)\n",
    "        e = emb_dict['e'].cuda()\n",
    "        y = emb_dict['y'].cuda()\n",
    "        g = emb_dict['g'].cuda()\n",
    "        w0 = emb_dict['w0'].cuda()\n",
    "        b0 = emb_dict['b0'].cuda()\n",
    "        logits = e @ w0.t() + b0\n",
    "        weights = compute_afr_weights(logits, y, gamma, True)\n",
    "        n_eff.append(1 / (weights ** 2).sum().item())\n",
    "        twga.append(this_twga)\n",
    "    twgas.append(twga)\n",
    "    n_effs.append(n_eff)\n",
    "twgas = np.array(twgas)\n",
    "n_effs = np.array(n_effs)\n",
    "\n",
    "mean_wgas = twgas.mean(axis=1) * 100\n",
    "std_wgas = twgas.std(axis=1) * 100\n",
    "mean_n_effs = n_effs.mean(axis=1)\n",
    "std_n_effs = n_effs.std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", font_scale=2.0, rc={\"lines.linewidth\": 3.0})\n",
    "sns.set_palette(\"Set1\")\n",
    "\n",
    "plt.figure(dpi=100, figsize=(8, 6))\n",
    "plt.fill_between(gammas, mean_wgas - std_wgas, mean_wgas + std_wgas, alpha=0.2)\n",
    "plt.plot(gammas, mean_wgas, label='AFR', marker='o', markersize=10)\n",
    "plt.xlabel(r'$\\gamma$')\n",
    "\n",
    "erm = 47.2\n",
    "\n",
    "plt.plot([0, max(gammas)], [erm, erm], '--', label='ERM')\n",
    "plt.ylim(bottom=40)\n",
    "plt.ylabel('Test WGA [%]')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../plots/celeba_wga_vs_gamma.pdf')\n",
    "\n",
    "\n",
    "plt.figure(dpi=100, figsize=(8, 6))\n",
    "plt.fill_between(gammas, mean_n_effs - std_n_effs, mean_n_effs + std_n_effs, alpha=0.2)\n",
    "plt.plot(gammas, mean_n_effs, marker='o', markersize=10)\n",
    "plt.xlabel(r'$\\gamma$')\n",
    "plt.ylabel(r'Effective # Samples')\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../plots/celeba_effsize_vs_gamma.pdf')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Label Efficiency / Down-sampled Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide an example for Waterbirds\n",
    "for seed in [0, 21, 42]:\n",
    "    for num_augs in [1]:\n",
    "        for val_prop in [0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5]:\n",
    "            args = SimpleNamespace(\n",
    "                    data_transform='NoAugWaterbirdsCelebATransform',\n",
    "                    dataset='SpuriousDataset',\n",
    "                    data_dir=wb_data_path,\n",
    "                    num_minority_groups_remove=0,\n",
    "                    test_data_dir=None,\n",
    "                    val_size=-1,\n",
    "                    mixup=False,\n",
    "                    batch_size=32,\n",
    "                    reweight_groups=False,\n",
    "                    reweight_classes=False,\n",
    "                    reweight_spurious=False,\n",
    "                    no_shuffle_train=False,\n",
    "                    finetune_on_val=False,\n",
    "                    dfr=False,\n",
    "                    pass_n=0,\n",
    "                    max_prop=1,\n",
    "                    train_prop=-0.2,\n",
    "                    val_prop=1, # subsampling happens in sweep function\n",
    "                )\n",
    "            sweep_config = {\n",
    "                \"n_trials\": 33 * 4,\n",
    "                \"num_epochs\": 500,\n",
    "                \"lr_choices\": [1e-2],\n",
    "                \"gamma_choices\": np.linspace(4, 20, 33),\n",
    "                \"reg_coeff_choices\": [0, 0.1, 0.2, 0.3],\n",
    "                \"balance_classes_choices\": [True],\n",
    "                \"early_stop\": 'none', # as explained in the paper, we do not use early stopping for the sweep\n",
    "                \"random\": False,\n",
    "                \"val_prop\": val_prop,\n",
    "                'sweep_name': f'val_prop_{val_prop}_seed{seed}'\n",
    "                }\n",
    "            sweep(WB_CKPTS, args, num_augs, sweep_config, load_emb, skip_if_done, seed=seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ca484d7210b95a2e9d854ba4e3641b0188ec8a1dc74d767b9af89b5f3db9867a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
