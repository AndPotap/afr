{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env OMP_NUM_THREADS=4\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../wilds_exps_utils/\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import pickle\n",
    "import copy\n",
    "from types import SimpleNamespace\n",
    "from wilds import get_dataset\n",
    "from wilds.common.data_loaders import get_eval_loader\n",
    "from wilds_configs import datasets as dataset_configs\n",
    "from wilds.datasets.wilds_dataset import WILDSSubset\n",
    "from wilds_models.initializer import initialize_model\n",
    "import wilds_transforms as transforms\n",
    "\n",
    "# from wilds_algorithms.initializer import infer_d_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from types import SimpleNamespace\n",
    "import tqdm\n",
    "import torch\n",
    "\n",
    "from transformers import BertConfig, BertForSequenceClassification\n",
    "\n",
    "import sys\n",
    "gdro_dir = '/home/pavel_i/projects/ssl_robustness/group_DRO'\n",
    "sys.path.append(gdro_dir)\n",
    "from gdro_data.data import prepare_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GROUPS = 6\n",
    "\n",
    "gdro_config = SimpleNamespace(\n",
    "    dataset='MultiNLI',\n",
    "    shift_type='confounder',\n",
    "    root_dir='/data/users/pavel_i/datasets/multinli',\n",
    "    augment_data=False,\n",
    "    gamma=0.1,\n",
    "    batch_size=128,\n",
    "    target_name='gold_label_random',\n",
    "    confounder_names=['sentence2_has_negation',],\n",
    "    model='bert',\n",
    "    fraction=1.,\n",
    ")\n",
    "reweighting_data, val_data, test_data = prepare_data(gdro_config, train=True)\n",
    "loader_kwargs = {'num_workers':4, 'pin_memory':True}\n",
    "\n",
    "# reweighting_data = val_data\n",
    "val_loader = val_data.get_loader(\n",
    "        train=False, reweight_groups=None, **loader_kwargs)\n",
    "test_loader = test_data.get_loader(\n",
    "        train=False, reweight_groups=None, **loader_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_predictions(feature_extractor, classifier, loader):\n",
    "    all_embeddings, all_predictions, all_y_true, all_metadata = [], [], [], []\n",
    "#     i = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y_true, metadata in tqdm.tqdm(loader):\n",
    "            input_ids = x[:, :, 0].cuda()\n",
    "            input_masks = x[:, :, 1].cuda()\n",
    "            segment_ids = x[:, :, 2].cuda()\n",
    "            embeddings = feature_extractor(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=input_masks,\n",
    "                    token_type_ids=segment_ids).logits\n",
    "            predictions = torch.argmax(classifier(embeddings), axis=1)\n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "            all_predictions.append(predictions.cpu())\n",
    "            all_y_true.append(y_true.cpu())\n",
    "            all_metadata.append(metadata)\n",
    "#             i += 1\n",
    "#             if i > 20:\n",
    "#                 break\n",
    "    all_embeddings = torch.cat(all_embeddings, axis=0)\n",
    "    all_predictions = torch.cat(all_predictions, axis=0)\n",
    "    all_y_true = torch.cat(all_y_true, axis=0)\n",
    "    all_metadata = torch.cat(all_metadata, axis=0)\n",
    "    return all_embeddings, all_predictions, all_y_true, all_metadata\n",
    "\n",
    "def save_emb(ckpt_path, seed, save_path):\n",
    "    reweighting_data, val_data, test_data = prepare_data(gdro_config, train=True)\n",
    "    loader_kwargs = {'num_workers':4, 'pin_memory':True}\n",
    "\n",
    "    # reweighting_data = val_data\n",
    "    val_loader = val_data.get_loader(\n",
    "            train=False, reweight_groups=None, **loader_kwargs)\n",
    "    test_loader = test_data.get_loader(\n",
    "            train=False, reweight_groups=None, **loader_kwargs)\n",
    "    dfr_reweighting_seed = seed\n",
    "    dfr_reweighting_frac = 0.2\n",
    "\n",
    "    print(f'Dropping DFR reweighting data, seed {dfr_reweighting_seed}')\n",
    "\n",
    "    idx = reweighting_data.dataset.indices.copy()\n",
    "    rng = np.random.default_rng(dfr_reweighting_seed)\n",
    "    rng.shuffle(idx)\n",
    "    n_train = int((1 - dfr_reweighting_frac) * len(idx))\n",
    "    reweighting_idx = idx[n_train:]\n",
    "\n",
    "    print(f'Original dataset size: {len(reweighting_data.dataset.indices)}')\n",
    "    reweighting_data.dataset = torch.utils.data.dataset.Subset(\n",
    "        reweighting_data.dataset.dataset,\n",
    "        indices=reweighting_idx)\n",
    "    print(f'New dataset size: {len(reweighting_data.dataset.indices)}')\n",
    "\n",
    "    reweighting_loader = reweighting_data.get_loader(\n",
    "            train=False, reweight_groups=None, **loader_kwargs)\n",
    "    model = torch.load(ckpt_path)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    classifier = model.classifier\n",
    "    model.classifier = torch.nn.Identity(classifier.in_features)\n",
    "\n",
    "    feature_extractor, classifier = model, classifier\n",
    "    reweighting_embeddings, reweighting_predictions, reweighting_y, reweighting_metadata = get_embeddings_predictions(\n",
    "            feature_extractor, classifier, reweighting_loader)\n",
    "    val_embeddings, val_predictions, val_y, val_metadata = get_embeddings_predictions(\n",
    "            feature_extractor, classifier, val_loader)\n",
    "    test_embeddings, test_predictions, test_y, test_metadata = get_embeddings_predictions(\n",
    "            feature_extractor, classifier, test_loader)\n",
    "    torch.save(\n",
    "        dict(\n",
    "            e=reweighting_embeddings, y=reweighting_y, pred=reweighting_predictions, m=reweighting_metadata,\n",
    "            test_e=test_embeddings, test_pred=test_predictions, test_y=test_y, test_m=test_metadata,\n",
    "            val_e=val_embeddings, val_pred=val_predictions, val_y=val_y, val_m=val_metadata,\n",
    "            w0 = classifier.weight.cpu(),\n",
    "            b0 = classifier.bias.cpu()\n",
    "        ),\n",
    "        save_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = '/home/shikai_q/multinli_ckpts/multinli' # TODO: change to your directory of checkpoints\n",
    "ckpts = ['erm_2', 'erm_1', 'erm_0', 'erm_dfrdrop0', 'erm_dfrdrop_1', 'erm_dfrdrop2'] # TODO: change to your checkpoints\n",
    "seeds = [0, 0, 0, 0, 1, 2]\n",
    "for ckpt, seed in zip(ckpts, seeds):\n",
    "    ckpt_path = f'{ckpt_dir}/{ckpt}/last_model.pth'\n",
    "    save_path = f'emb/multinli/{ckpt}.pt'\n",
    "    save_emb(ckpt_path, seed, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"emb/multinli/erm_0.pt\")\n",
    "reweighting_embeddings = ckpt[\"e\"]\n",
    "reweighting_y = ckpt[\"y\"]\n",
    "reweighting_predictions = ckpt[\"pred\"]\n",
    "reweighting_metadata = ckpt[\"m\"]\n",
    "test_embeddings = ckpt[\"test_e\"]\n",
    "test_predictions = ckpt[\"test_pred\"]\n",
    "test_y = ckpt[\"test_y\"]\n",
    "test_metadata = ckpt[\"test_m\"]\n",
    "val_embeddings = ckpt[\"val_e\"]\n",
    "val_predictions = ckpt[\"val_pred\"]\n",
    "val_y = ckpt[\"val_y\"]\n",
    "val_metadata = ckpt[\"val_m\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
